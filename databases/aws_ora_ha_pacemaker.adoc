---
sidebar: sidebar 
permalink: databases/aws_ora_ha_pacemaker.html 
keywords: Database, Oracle, AWS, EC2, FSx ONTAP, HA, Pacemaker 
summary: 'Diese Lösung bietet einen Überblick und Details zur Aktivierung von Oracle High Availability (HA) in AWS EC2 mit Pacemaker-Clustering auf Redhat Enterprise Linux (RHEL) und Amazon FSX ONTAP für die Datenbank-Storage-HA über NFS-Protokoll.' 
---
= TR-4998: Oracle HA in AWS EC2 mit Pacemaker Clustering und FSX ONTAP
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


Allen Cao, Niyaz Mohamed, NetApp

[role="lead"]
Diese Lösung bietet einen Überblick und Details zur Aktivierung von Oracle High Availability (HA) in AWS EC2 mit Pacemaker-Clustering auf Redhat Enterprise Linux (RHEL) und Amazon FSX ONTAP für die Datenbank-Storage-HA über NFS-Protokoll.



== Zweck

Viele Kunden, die Oracle selbst managen und in der Public Cloud ausführen möchten, müssen einige Herausforderungen meistern. Eine dieser Herausforderungen ist die Bereitstellung von Hochverfügbarkeit für die Oracle Datenbank. Traditionell setzen Oracle Kunden für die aktiv/aktiv-Transaktionsunterstützung auf mehreren Cluster-Knoten auf eine Oracle Datenbankfunktion namens „Real Application Cluster“ oder RAC. Ein ausgefallener Node würde die Anwendungsverarbeitung nicht zum Stillstand bringen. Leider ist die Implementierung von Oracle RAC in vielen gängigen Public Clouds wie AWS EC2 nicht sofort verfügbar oder wird sie nicht unterstützt. Durch die Nutzung des integrierten Pacemaker-Clustering (PCS) in RHEL und Amazon FSX ONTAP können Kunden eine praktikable Alternative ohne Oracle RAC-Lizenzkosten für aktiv-Passiv-Clustering auf Rechner und Speicher zur Unterstützung unternehmenskritischer Oracle-Datenbank-Workloads in der AWS-Cloud erzielen.

In dieser Dokumentation werden die Details zur Einrichtung des Pacemaker-Clustering unter RHEL, zur Bereitstellung von Oracle-Datenbanken auf EC2 und Amazon FSX ONTAP mit NFS-Protokoll, zur Konfiguration von Oracle-Ressourcen in Pacemaker für HA und zum Abschließen der Demo mit einer Validierung unter den am häufigsten angetroffenen HA-Szenarien beschrieben. Darüber hinaus bietet die Lösung mit dem UI-Tool NetApp SnapCenter Informationen zum schnellen Sichern, Wiederherstellen und Klonen von Oracle-Datenbanken.

Diese Lösung eignet sich für folgende Anwendungsfälle:

* Einrichtung und Konfiguration von Pacemaker-HA-Clustering in RHEL
* Oracle Database HA-Implementierung in AWS EC2 und Amazon FSX ONTAP




== Zielgruppe

Diese Lösung ist für folgende Personen gedacht:

* Ein DBA, der Oracle in AWS EC2 und Amazon FSX ONTAP implementieren möchte.
* Ein Database Solution Architect, der Oracle-Workloads in AWS EC2 und Amazon FSX ONTAP testen möchte.
* Ein Storage-Administrator, der eine Oracle Datenbank in AWS EC2 und Amazon FSX ONTAP implementieren und managen möchte.
* Ein Applikationseigentümer, der eine Oracle Database in AWS EC2 und Amazon FSX ONTAP einrichten möchte.




== Test- und Validierungsumgebung der Lösung

Die Lösung wurde in einer Testumgebung getestet und validiert. Siehe Abschnitt <<Wichtige Faktoren für die Implementierung>> Finden Sie weitere Informationen.



=== Der Netapp Architektur Sind

image:aws_ora_fsx_ec2_pcs_architecture.png["Dieses Bild zeigt eine detaillierte Ansicht der Oracle Hochverfügbarkeit in AWS EC2 mit Pacemaker Clustering und FSX ONTAP."]



=== Hardware- und Softwarekomponenten

[cols="33%, 33%, 33%"]
|===


3+| *Hardware* 


| Amazon FSX ONTAP-Storage | Aktuelle Version von AWS angeboten | Single-AZ in US-East-1, Kapazität 1024 gib, Durchsatz 128 MB/s 


| EC2 Instanzen für DB-Server | t2.xlarge/4vCPU/16G | Zwei EC2 T2 xlarge EC2-Instanzen, eine als primärer DB-Server und die andere als Standby-DB-Server 


| VM für Ansible Controller | 4 vCPUs, 16 gib RAM | Eine Linux VM zum Ausführen der automatisierten Bereitstellung von AWS EC2/FSX und Oracle auf NFS 


3+| *Software* 


| Redhat Linux | RHEL Linux 8.6 (LVM) – x64 Gen2 | Bereitstellung der RedHat Subscription für Tests 


| Oracle Datenbank | Version 19.18 | RU-Patch p34765931_190000_Linux-x86-64.zip angewendet 


| Oracle OPatch | Version 12.2.0.1.36 | Neuestes Patch p6880880_190000_Linux-x86-64.zip 


| Schrittmacher | Version 0.10.18 | High Availability Add-on für RHEL 8.0 von RedHat 


| NFS | Version 3.0 | Oracle dNFS aktiviert 


| Ansible | Kern 2.16.2 | Python 3.6.8 
|===


=== Aktive/passive Oracle-Konfiguration in der AWS EC2/FSX Lab-Umgebung

[cols="33%, 33%, 33%"]
|===


3+|  


| *Server* | * Datenbank* | *DB-Speicher* 


| Primärer Knoten: Orapm01/ip-172.30.15.111 | NTAP(NTAP_PDB1,NTAP_PDB2,NTAP_PDB3) | /U01, /u02, /u03 NFS-Mounts sind auf Amazon FSX ONTAP Volumes möglich 


| Standby-Node: Orapm02/ip-172.30.15.5 | NTAP(NTAP_PDB1,NTAP_PDB2,NTAP_PDB3) BEI FAILOVER | /U01, /u02, /u03 NFS-Mounts bei Failover 
|===


=== Wichtige Faktoren für die Implementierung

* * Amazon FSX ONTAP HA.* Amazon FSX ONTAP wird standardmäßig in einem HA-Paar von Storage-Controllern in einer oder mehreren Verfügbarkeitszonen bereitgestellt. Sie bietet aktiv/Passiv-Storage-Redundanz für geschäftskritische Datenbank-Workloads. Der Storage-Failover ist für den Endanwender transparent. Im Falle eines Storage-Failovers sind keinerlei Benutzereingriffe erforderlich.
* *PCS Ressourcengruppe und Ressourcenbestellung.* Mit einer Ressourcengruppe können mehrere Ressourcen mit Abhängigkeit auf demselben Clusterknoten ausgeführt werden. Die Ressourcenreihenfolge erzwingt die Ressourcen-Startreihenfolge und die Reihenfolge des Herunterfahrens in umgekehrter Reihenfolge.
* *Bevorzugter Knoten.* Der Pacemaker-Cluster wird absichtlich in aktiv/Passiv-Clustering bereitgestellt (keine Anforderung von Pacemaker) und ist mit FSX ONTAP-Clustering synchronisiert. Die aktive EC2-Instanz wird als bevorzugter Knoten für Oracle-Ressourcen konfiguriert, wenn diese mit einer Speicherortbeschränkung verfügbar sind.
* *Fence Delay auf Standby-Knoten.* In einem zwei-Knoten-PCS-Cluster wird ein Quorum künstlich als 1 festgelegt. Bei einem Kommunikationsproblem zwischen den Cluster-Nodes könnte jeder Node versuchen, den anderen Node abzuzauern, was möglicherweise zu Datenbeschädigungen führen kann. Durch das Einrichten einer Verzögerung auf dem Standby-Node wird das Problem verringert und der primäre Node kann weiterhin Dienste bereitstellen, während der Standby-Node abgetrennt ist.
* *Multi az Deployment-Überlegungen.* Die Lösung wird in einer einzigen Verfügbarkeitszone implementiert und validiert. Für die Multi-az-Implementierung sind zusätzliche AWS-Netzwerkressourcen erforderlich, um die PCS-Floating-IP zwischen den Verfügbarkeitszonen zu verschieben.
* *Speicherlayout für Oracle-Datenbanken.* In dieser Lösungsdemonstration stellen wir drei Datenbank-Volumes für die Testdatenbank NTAP bereit, um Oracle Binärdaten, Daten und Protokolle zu hosten. Die Volumes werden auf dem Oracle DB Server als /u01 - Binary, /u02 - Data und /u03 - log via NFS eingebunden. Dual-Control-Dateien werden auf den Mount-Punkten /u02 und /u03 für Redundanz konfiguriert.
* *DNFS-Konfiguration.* mit dNFS (verfügbar seit Oracle 11g) kann eine auf einer DB VM laufende Oracle-Datenbank deutlich mehr I/O-Vorgänge erzeugen als der native NFS-Client. Die automatisierte Oracle-Implementierung konfiguriert dNFS standardmäßig auf NFSv3.
* *Datenbanksicherung.* NetApp bietet eine SnapCenter Software Suite für Datenbank-Backup, -Wiederherstellung und -Klonen mit einer benutzerfreundlichen Benutzeroberfläche. NetApp empfiehlt die Implementierung eines solchen Managementtools, um Snapshots schnell (unter einer Minute), schnelle Datenbank-Restores (in Minuten) und Datenbankklone zu ermöglichen.




== Lösungsimplementierung

Die folgenden Abschnitte enthalten Schritt-für-Schritt-Anleitungen für die Implementierung und Konfiguration von Oracle Datenbank-HA in AWS EC2 mit Pacemaker-Clustering und Amazon FSX ONTAP zur Sicherung von Datenbank-Storage.



=== Voraussetzungen für die Bereitstellung

[%collapsible]
====
Die Bereitstellung erfordert die folgenden Voraussetzungen.

. Es wurde ein AWS Konto eingerichtet, und die erforderlichen VPC und Netzwerksegmente wurden in Ihrem AWS Konto erstellt.
. Stellen Sie eine Linux VM als Ansible-Controller-Node mit der neuesten Version von Ansible und Git bereit. Details finden Sie unter folgendem Link: link:../automation/getting-started.html["Erste Schritte mit der Automatisierung von NetApp Lösungen"^] In Abschnitt -
`Setup the Ansible Control Node for CLI deployments on RHEL / CentOS` Oder
`Setup the Ansible Control Node for CLI deployments on Ubuntu / Debian`.
+
Aktivieren Sie SSH-Authentifizierung für öffentlichen/privaten Schlüssel zwischen dem Ansible Controller und den VMs der EC2-Instanz.



====


=== Bereitstellung von EC2 Instanzen und Amazon FSX ONTAP Storage-Cluster

[%collapsible]
====
Obwohl die EC2-Instanz und Amazon FSX ONTAP manuell über die AWS-Konsole bereitgestellt werden können, empfiehlt es sich, das auf NetApp Terraform basierende Automatisierungs-Toolkit zu verwenden, um die Bereitstellung von EC2-Instanzen und dem FSX ONTAP Storage-Cluster zu automatisieren. Im Folgenden finden Sie die detaillierten Verfahren.

. Klonen Sie von der Controller-VM AWS CloudShell oder Ansible eine Kopie des Automatisierungs-Toolkits für EC2 und FSX ONTAP.
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_aws_fsx_ec2_deploy.git
----
+

NOTE: Wenn das Toolkit nicht über AWS CloudShell ausgeführt wird, ist die AWS-CLI-Authentifizierung über Ihr AWS-Konto mit Zugriff auf das AWS-Benutzerkonto und dem Schlüsselpaar für den geheimen Schlüssel erforderlich.

. Lesen Sie die im Toolkit enthaltene Datei readme.md durch. Überarbeiten Sie main.tf und die zugehörigen Parameterdateien nach Bedarf für die erforderlichen AWS-Ressourcen.
+
....
An example of main.tf:

resource "aws_instance" "orapm01" {
  ami                           = var.ami
  instance_type                 = var.instance_type
  subnet_id                     = var.subnet_id
  key_name                      = var.ssh_key_name

  root_block_device {
    volume_type                 = "gp3"
    volume_size                 = var.root_volume_size
  }

  tags = {
    Name                        = var.ec2_tag1
  }
}

resource "aws_instance" "orapm02" {
  ami                           = var.ami
  instance_type                 = var.instance_type
  subnet_id                     = var.subnet_id
  key_name                      = var.ssh_key_name

  root_block_device {
    volume_type                 = "gp3"
    volume_size                 = var.root_volume_size
  }

  tags = {
    Name                        = var.ec2_tag2
  }
}

resource "aws_fsx_ontap_file_system" "fsx_01" {
  storage_capacity              = var.fs_capacity
  subnet_ids                    = var.subnet_ids
  preferred_subnet_id           = var.preferred_subnet_id
  throughput_capacity           = var.fs_throughput
  fsx_admin_password            = var.fsxadmin_password
  deployment_type               = var.deployment_type

  disk_iops_configuration {
    iops                        = var.iops
    mode                        = var.iops_mode
  }

  tags                          = {
    Name                        = var.fsx_tag
  }
}

resource "aws_fsx_ontap_storage_virtual_machine" "svm_01" {
  file_system_id                = aws_fsx_ontap_file_system.fsx_01.id
  name                          = var.svm_name
  svm_admin_password            = var.vsadmin_password
}

....
. Validieren und Ausführen des Terraform-Plans Bei einer erfolgreichen Ausführung würden zwei EC2-Instanzen und Ein FSX ONTAP Storage-Cluster im AWS-Zielkonto erstellt. Der Automatisierungsausgang zeigt die IP-Adresse der EC2-Instanz und die Endpunkte des FSX ONTAP-Clusters an.
+
[source, cli]
----
terraform plan -out=main.plan
----
+
[source, cli]
----
terraform apply main.plan
----


Damit sind die EC2-Instanzen und die FSX ONTAP-Bereitstellung für Oracle abgeschlossen.

====


=== Pacemaker-Cluster-Einrichtung

[%collapsible]
====
Das High Availability Add-On für RHEL ist ein geclustertes System, das Zuverlässigkeit, Skalierbarkeit und Verfügbarkeit für kritische Produktionsservices wie Oracle-Datenbankservices bietet. In diesem Anwendungsbeispiel wird ein Pacemaker-Cluster mit zwei Knoten eingerichtet und konfiguriert, um die Hochverfügbarkeit einer Oracle-Datenbank in einem aktiv/Passiv-Clustering-Szenario zu unterstützen.  

Melden Sie sich als ec2-User bei ec2-Instanzen an und führen Sie folgende Aufgaben auf `both` EC2-Instanzen durch:

. Entfernen Sie den AWS Red hat Update Infrastructure (RHUI)-Client.
+
[source, cli]
----
sudo -i yum -y remove rh-amazon-rhui-client*
----
. Registrieren Sie die EC2-Instanz-VMs bei Red hat.
+
[source, cli]
----
sudo subscription-manager register --username xxxxxxxx --password 'xxxxxxxx' --auto-attach
----
. Aktivierung von RHEL-RMS mit hoher Verfügbarkeit
+
[source, cli]
----
sudo subscription-manager config --rhsm.manage_repos=1
----
+
[source, cli]
----
sudo subscription-manager repos --enable=rhel-8-for-x86_64-highavailability-rpms
----
. Den Schrittmacher und die Anschlaglineal einbauen.
+
[source, cli]
----
sudo yum update -y
----
+
[source, cli]
----
sudo yum install pcs pacemaker fence-agents-aws
----
. Erstellen Sie ein Passwort für hacluster Benutzer auf allen Cluster-Knoten. Verwenden Sie für alle Nodes dasselbe Passwort.
+
[source, cli]
----
sudo passwd hacluster
----
. Starten Sie den PC-Service und aktivieren Sie ihn beim Start.
+
[source, cli]
----
sudo systemctl start pcsd.service
----
+
[source, cli]
----
sudo systemctl enable pcsd.service
----
. pcsd-Dienst validieren.
+
[source, cli]
----
sudo systemctl status pcsd
----
+
....
[ec2-user@ip-172-30-15-5 ~]$ sudo systemctl status pcsd
● pcsd.service - PCS GUI and remote configuration interface
   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2024-09-10 18:50:22 UTC; 33s ago
     Docs: man:pcsd(8)
           man:pcs(8)
 Main PID: 65302 (pcsd)
    Tasks: 1 (limit: 100849)
   Memory: 24.0M
   CGroup: /system.slice/pcsd.service
           └─65302 /usr/libexec/platform-python -Es /usr/sbin/pcsd

Sep 10 18:50:21 ip-172-30-15-5.ec2.internal systemd[1]: Starting PCS GUI and remote configuration interface...
Sep 10 18:50:22 ip-172-30-15-5.ec2.internal systemd[1]: Started PCS GUI and remote configuration interface.

....
. Cluster-Nodes zu Hostdateien hinzufügen.
+
[source, cli]
----
sudo vi /etc/hosts
----
+
....
[ec2-user@ip-172-30-15-5 ~]$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

# cluster nodes
172.30.15.111   ip-172-30-15-111.ec2.internal
172.30.15.5     ip-172-30-15-5.ec2.internal

....
. Awscli für Verbindung zum AWS-Konto installieren und konfigurieren
+
[source, cli]
----
sudo yum install awscli
----
+
[source, cli]
----
sudo aws configure
----
+
....
[ec2-user@ip-172-30-15-111 ]# sudo aws configure
AWS Access Key ID [None]: XXXXXXXXXXXXXXXXX
AWS Secret Access Key [None]: XXXXXXXXXXXXXXXX
Default region name [None]: us-east-1
Default output format [None]: json

....
. Installieren Sie das Resource-Agents-Paket, wenn es nicht bereits installiert ist.
+
[source, cli]
----
sudo yum install resource-agents
----


 `only one`Führen Sie auf dem Clusterknoten die folgenden Aufgaben aus, um ein PCs-Cluster zu erstellen.

. Authentifizieren Sie den PC-Benutzer hacluster.
+
[source, cli]
----
sudo pcs host auth ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs host auth ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
Username: hacluster
Password:
ip-172-30-15-111.ec2.internal: Authorized
ip-172-30-15-5.ec2.internal: Authorized

....
. Erstellen Sie den PC-Cluster.
+
[source, cli]
----
sudo pcs cluster setup ora_ec2nfsx ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs cluster setup ora_ec2nfsx ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal
No addresses specified for host 'ip-172-30-15-5.ec2.internal', using 'ip-172-30-15-5.ec2.internal'
No addresses specified for host 'ip-172-30-15-111.ec2.internal', using 'ip-172-30-15-111.ec2.internal'
Destroying cluster on hosts: 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'...
ip-172-30-15-5.ec2.internal: Successfully destroyed cluster
ip-172-30-15-111.ec2.internal: Successfully destroyed cluster
Requesting remove 'pcsd settings' from 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful removal of the file 'pcsd settings'
ip-172-30-15-5.ec2.internal: successful removal of the file 'pcsd settings'
Sending 'corosync authkey', 'pacemaker authkey' to 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'corosync authkey'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'pacemaker authkey'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'corosync authkey'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'pacemaker authkey'
Sending 'corosync.conf' to 'ip-172-30-15-111.ec2.internal', 'ip-172-30-15-5.ec2.internal'
ip-172-30-15-111.ec2.internal: successful distribution of the file 'corosync.conf'
ip-172-30-15-5.ec2.internal: successful distribution of the file 'corosync.conf'
Cluster has been successfully set up.

....
. Aktivieren Sie den Cluster.
+
[source, cli]
----
sudo pcs cluster enable --all
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs cluster enable --all
ip-172-30-15-5.ec2.internal: Cluster Enabled
ip-172-30-15-111.ec2.internal: Cluster Enabled

....
. Starten und validieren Sie den Cluster.
+
[source, cli]
----
sudo pcs cluster start --all
----
+
[source, cli]
----
sudo pcs status
----
+
....
[ec2-user@ip-172-30-15-111 ~]$ sudo pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Wed Sep 11 15:43:23 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Wed Sep 11 15:43:06 2024 by hacluster via hacluster on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]


Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....


Hiermit sind die Pacemaker-Cluster-Einrichtung und die Erstkonfiguration abgeschlossen.

====


=== Konfiguration des Pacemaker-Cluster-Fencing

[%collapsible]
====
Die Schrittmacherfencing-Konfiguration ist für einen Produktionscluster obligatorisch. Die Lösung sorgt dafür, dass ein defekter Node auf Ihrem AWS EC2 Cluster automatisch isoliert wird. Auf diese Weise wird verhindert, dass der Node die Ressourcen des Clusters nutzt, die Funktionalität des Clusters beeinträchtigt oder gemeinsam genutzte Daten beschädigt. In diesem Abschnitt wird die Konfiguration von Cluster-Fencing mit dem Fechtagenten Fence_aws beschrieben.

. Geben Sie als Root-Benutzer die folgende AWS-Metadatenabfrage ein, um die Instanz-ID für jeden EC2-Instanzknoten zu erhalten.
+
[source, cli]
----
echo $(curl -s http://169.254.169.254/latest/meta-data/instance-id)
----
+
....
[root@ip-172-30-15-111 ec2-user]# echo $(curl -s http://169.254.169.254/latest/meta-data/instance-id)
i-0d8e7a0028371636f

or just get instance-id from AWS EC2 console
....
. Geben Sie den folgenden Befehl ein, um das Lineal zu konfigurieren. Verwenden Sie den Befehl pcmk_Host_map, um den RHEL-Hostnamen der Instanz-ID zuzuordnen. Verwenden Sie den AWS-Zugriffsschlüssel und den AWS-geheimen Zugriffsschlüssel des AWS-Benutzerkontos, das Sie zuvor für die AWS-Authentifizierung verwendet haben.
+
[source, cli]
----
sudo pcs stonith \
create clusterfence fence_aws access_key=XXXXXXXXXXXXXXXXX secret_key=XXXXXXXXXXXXXXXXXX \
region=us-east-1 pcmk_host_map="ip-172-30-15-111.ec2.internal:i-0d8e7a0028371636f;ip-172-30-15-5.ec2.internal:i-0bc54b315afb20a2e" \
power_timeout=240 pcmk_reboot_timeout=480 pcmk_reboot_retries=4
----
. Überprüfen Sie die Fencing-Konfiguration.
+
[source, cli]
----
pcs status
----
+
....
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Wed Sep 11 21:17:18 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Wed Sep 11 21:16:40 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 1 resource instance configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. Setzen Sie stonith-Action auf die Option off statt Reboot auf Cluster-Ebene.
+
[source, cli]
----
pcs property set stonith-action=off
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs property config
Cluster Properties:
 cluster-infrastructure: corosync
 cluster-name: ora_ec2nfsx
 dc-version: 2.1.7-5.1.el8_10-0f7f88312
 have-watchdog: false
 last-lrm-refresh: 1726257586
 stonith-action: off

....
+

NOTE: Wenn stonith-Action auf aus gesetzt ist, wird der umzäunte Clusterknoten zunächst heruntergefahren. Nach dem in stonith Power_Timeout (240 Sekunden) definierten Zeitraum wird der umzäunte Knoten neu gestartet und dem Cluster wieder hinzugefügt.

. Legen Sie für den Standby-Node die Zaunverzögerung auf 10 Sekunden fest.
+
[source, cli]
----
pcs stonith update clusterfence pcmk_delay_base="ip-172-30-15-111.ec2.internal:0;ip-172-30-15-5.ec2.internal:10s"
----
+
....
[root@ip-172-30-15-111 ec2-user]# pcs stonith config
Resource: clusterfence (class=stonith type=fence_aws)
  Attributes: clusterfence-instance_attributes
    access_key=XXXXXXXXXXXXXXXX
    pcmk_delay_base=ip-172-30-15-111.ec2.internal:0;ip-172-30-15-5.ec2.internal:10s
    pcmk_host_map=ip-172-30-15-111.ec2.internal:i-0d8e7a0028371636f;ip-172-30-15-5.ec2.internal:i-0bc54b315afb20a2e
    pcmk_reboot_retries=4
    pcmk_reboot_timeout=480
    power_timeout=240
    region=us-east-1
    secret_key=XXXXXXXXXXXXXXXX
  Operations:
    monitor: clusterfence-monitor-interval-60s
      interval=60s

....



NOTE: Führen Sie `pcs stonith refresh` einen Befehl aus, um den stonith-Zaunagenten zu aktualisieren oder fehlgeschlagene stonith-Ressourcenaktionen zu löschen.

====


=== Stellen Sie Oracle-Datenbanken im PC-Cluster bereit

[%collapsible]
====
Wir empfehlen die Nutzung des von NetApp bereitgestellten Ansible-Playbooks, um Datenbankinstallations- und -Konfigurationsaufgaben mit vordefinierten Parametern auf dem PCS-Cluster auszuführen. Für diese automatisierte Oracle-Bereitstellung müssen drei benutzerdefinierte Parameterdateien vor der Ausführung des Playbooks vom Benutzer eingegeben werden.

* Hosts: Legen Sie Ziele fest, für die das Automatisierungs-Playbook ausgeführt wird.
* vars/vars.yml - die globale Variablendatei, die Variablen definiert, die für alle Ziele gelten.
* Host_VARs/Host_Name.yml - die lokale Variablendatei, die Variablen definiert, die nur auf ein benanntes Ziel angewendet werden. In unserem Anwendungsbeispiel handelt es sich um die Oracle DB-Server.


Zusätzlich zu diesen benutzerdefinierten Variablendateien gibt es mehrere standardmäßige Variablendateien, die Standardparameter enthalten, die nicht geändert werden müssen, sofern dies nicht erforderlich ist. Im Folgenden werden Details zur automatisierten Oracle-Implementierung in AWS EC2 und FSX ONTAP in einer PCS-Clustering-Konfiguration angezeigt.

. Klonen Sie über das Ansible Controller-Admin-Home-Verzeichnis eine Kopie des NetApp Toolkit zur Automatisierung der Implementierung von Oracle für NFS.
+
[source, cli]
----
git clone https://bitbucket.ngage.netapp.com/scm/ns-bb/na_oracle_deploy_nfs.git
----
+

NOTE: Der Ansible-Controller kann sich in derselben VPC wie die EC2-Instanz der Datenbank oder on-Premises befinden, sofern zwischen ihnen eine Netzwerkverbindung besteht.

. Geben Sie die benutzerdefinierten Parameter in Host-Parameterdateien ein. Im Folgenden finden Sie ein Beispiel für eine typische Konfiguration der Host-Datei.
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat hosts
#Oracle hosts
[oracle]
orapm01 ansible_host=172.30.15.111 ansible_ssh_private_key_file=ec2-user.pem
orapm02 ansible_host=172.30.15.5 ansible_ssh_private_key_file=ec2-user.pem

....
. Geben Sie die benutzerdefinierten Parameter in den Parameterdateien vars/vars.yml ein. Im Folgenden finden Sie ein Beispiel für eine typische Konfiguration der Datei vars.yml.
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat vars/vars.yml
######################################################################
###### Oracle 19c deployment user configuration variables       ######
###### Consolidate all variables from ONTAP, linux and oracle   ######
######################################################################

###########################################
### ONTAP env specific config variables ###
###########################################

# Prerequisite to create three volumes in NetApp ONTAP storage from System Manager or cloud dashboard with following naming convention:
# db_hostname_u01 - Oracle binary
# db_hostname_u02 - Oracle data
# db_hostname_u03 - Oracle redo
# It is important to strictly follow the name convention or the automation will fail.


###########################################
### Linux env specific config variables ###
###########################################

redhat_sub_username: xxxxxxxx
redhat_sub_password: "xxxxxxxx"


####################################################
### DB env specific install and config variables ###
####################################################

# Database domain name
db_domain: ec2.internal

# Set initial password for all required Oracle passwords. Change them after installation.
initial_pwd_all: "xxxxxxxx"

....
. Geben Sie die benutzerdefinierten Parameter in den Parameterdateien Host_vars/Host_Name.yml ein. Im Folgenden finden Sie ein Beispiel für eine typische Dateikonfiguration von Host_VARs/Host_Name.yml.
+
....

[admin@ansiblectl na_oracle_deploy_nfs]$ cat host_vars/orapm01.yml
# User configurable Oracle host specific parameters

# Database SID. By default, a container DB is created with 3 PDBs within the CDB
oracle_sid: NTAP

# CDB is created with SGA at 75% of memory_limit, MB. Consider how many databases to be hosted on the node and
# how much ram to be allocated to each DB. The grand total of SGA should not exceed 75% available RAM on node.
memory_limit: 8192

# Local NFS lif ip address to access database volumes
nfs_lif: 172.30.15.95

....
+

NOTE: die nfs_LIF-Adresse kann aus der Ausgabe der FSX ONTAP-Cluster-Endpunkte aus der automatisierten EC2- und FSX ONTAP-Implementierung im vorherigen Abschnitt abgerufen werden.

. Erstellen Sie Datenbank-Volumes über die AWS FSX-Konsole. Stellen Sie sicher, dass Sie den Hostnamen des primären PCS-Node (orapm01) als Präfix für die Volumes verwenden, wie unten gezeigt.
+
image:aws_ora_fsx_ec2_pcs_01.png["Dieses Image bietet die Bereitstellung von Amazon FSX ONTAP-Volumes über die AWS FSX Konsole"] image:aws_ora_fsx_ec2_pcs_02.png["Dieses Image bietet die Bereitstellung von Amazon FSX ONTAP-Volumes über die AWS FSX Konsole"] image:aws_ora_fsx_ec2_pcs_03.png["Dieses Image bietet die Bereitstellung von Amazon FSX ONTAP-Volumes über die AWS FSX Konsole"] image:aws_ora_fsx_ec2_pcs_04.png["Dieses Image bietet die Bereitstellung von Amazon FSX ONTAP-Volumes über die AWS FSX Konsole"] image:aws_ora_fsx_ec2_pcs_05.png["Dieses Image bietet die Bereitstellung von Amazon FSX ONTAP-Volumes über die AWS FSX Konsole"]

. Stellen Sie die folgenden Oracle 19c-Installationsdateien auf dem primären PC-Knoten EC2-Instanz ip-172-30-15-111.ec2.internal /tmp/Archive-Verzeichnis mit 777-Berechtigung auf.
+
....
installer_archives:
  - "LINUX.X64_193000_db_home.zip"
  - "p34765931_190000_Linux-x86-64.zip"
  - "p6880880_190000_Linux-x86-64.zip"
....
. Führen Sie Playbook für Linux config für `all nodes`aus.
+
[source, cli]
----
ansible-playbook -i hosts 2-linux_config.yml -u ec2-user -e @vars/vars.yml
----
+
....
[admin@ansiblectl na_oracle_deploy_nfs]$ ansible-playbook -i hosts 2-linux_config.yml -u ec2-user -e @vars/vars.yml

PLAY [Linux Setup and Storage Config for Oracle] ****************************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ******************************************************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]
ok: [orapm02]

TASK [linux : Configure RedHat 7 for Oracle DB installation] ****************************************************************************************************************************************************************************************************************************************************************
skipping: [orapm01]
skipping: [orapm02]

TASK [linux : Configure RedHat 8 for Oracle DB installation] ****************************************************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/linux/tasks/rhel8_config.yml for orapm01, orapm02

TASK [linux : Register subscriptions for RedHat Server] *********************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]
ok: [orapm02]
.
.
.
....
. Playbook für oracle config ausführen `only on primary node` (Standby-Node in Hostdatei kommentieren)
+
[source, cli]
----
ansible-playbook -i hosts 4-oracle_config.yml -u ec2-user -e @vars/vars.yml --skip-tags "enable_db_start_shut"
----
+
....
[admin@ansiblectl na_oracle_deploy_nfs]$ ansible-playbook -i hosts 4-oracle_config.yml -u ec2-user -e @vars/vars.yml --skip-tags "enable_db_start_shut"

PLAY [Oracle installation and configuration] ********************************************************************************************************************************************************************************************************************************************************************************

TASK [Gathering Facts] ******************************************************************************************************************************************************************************************************************************************************************************************************
ok: [orapm01]

TASK [oracle : Oracle software only install] ********************************************************************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/oracle/tasks/oracle_install.yml for orapm01

TASK [oracle : Create mount points for NFS file systems / Mount NFS file systems on Oracle hosts] ***************************************************************************************************************************************************************************************************************************
included: /home/admin/na_oracle_deploy_nfs/roles/oracle/tasks/oracle_mount_points.yml for orapm01

TASK [oracle : Create mount points for NFS file systems] ********************************************************************************************************************************************************************************************************************************************************************
changed: [orapm01] => (item=/u01)
changed: [orapm01] => (item=/u02)
changed: [orapm01] => (item=/u03)
.
.
.
....
. Nachdem die Datenbank bereitgestellt wurde, hängt Kommentar out /u01, /u02, /u03 in /etc/fstab auf dem primären Knoten ein, da die Bereitstellungspunkte nur von PCS verwaltet werden.
+
[source, cli]
----
sudo vi /etc/fstab
----
+
....

[root@ip-172-30-15-111 ec2-user]# cat /etc/fstab
UUID=eaa1f38e-de0f-4ed5-a5b5-2fa9db43bb38       /       xfs     defaults        0       0
/mnt/swapfile swap swap defaults 0 0
#172.30.15.95:/orapm01_u01 /u01 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
#172.30.15.95:/orapm01_u02 /u02 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0
#172.30.15.95:/orapm01_u03 /u03 nfs rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536 0 0

....
. Kopieren Sie /etc/oratab /etc/oraInst.loc, /Home/oracle/.bash_profile in den Standby-Node. Stellen Sie sicher, dass die Dateieigentümerschaft und -Berechtigungen ordnungsgemäß erhalten bleiben.
. Beenden Sie die Datenbank, den Listener und umount /u01, /u02, /u03 auf dem primären Knoten.
+
....

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Wed Sep 18 16:51:02 UTC 2024
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Wed Sep 18 16:51:16 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> shutdown immediate;

SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0
[oracle@ip-172-30-15-111 ~]$ lsnrctl stop listener.ntap

[oracle@ip-172-30-15-111 ~]$ exit
logout
[root@ip-172-30-15-111 ec2-user]# umount /u01
[root@ip-172-30-15-111 ec2-user]# umount /u02
[root@ip-172-30-15-111 ec2-user]# umount /u03

....
. Erstellen Sie Bereitstellungspunkte auf dem Standby-Node ip-172-30-15-5.
+
[source, cli]
----
mkdir /u01
mkdir /u02
mkdir /u03
----
. FSX ONTAP-Datenbankvolumes auf dem Standby-Node ip-172-30-15-5 mounten.
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u01 /u01 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u02 /u02 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
[source, cli]
----
mount -t nfs 172.30.15.95:/orapm01_u03 /u03 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
----
+
....

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03

....
. Geändert zu oracle-Benutzer, binär neu verknüpfen.
+
....

[root@ip-172-30-15-5 ec2-user]# su - oracle
Last login: Thu Sep 12 18:09:03 UTC 2024 on pts/0
[oracle@ip-172-30-15-5 ~]$ env | grep ORA
ORACLE_SID=NTAP
ORACLE_HOME=/u01/app/oracle/product/19.0.0/NTAP
[oracle@ip-172-30-15-5 ~]$ cd $ORACLE_HOME/bin
[oracle@ip-172-30-15-5 bin]$ ./relink
writing relink log to: /u01/app/oracle/product/19.0.0/NTAP/install/relinkActions2024-09-12_06-21-40PM.log

....
. Kopieren Sie dnfs lib zurück in den odm-Ordner. Die dfns-Bibliotheksdatei kann durch die erneute Verknüpfung verloren gehen.
+
....

[oracle@ip-172-30-15-5 odm]$ cd /u01/app/oracle/product/19.0.0/NTAP/rdbms/lib/odm
[oracle@ip-172-30-15-5 odm]$ cp ../../../lib/libnfsodm19.so .

....
. Starten Sie die Datenbank zur Validierung auf dem Standby-Node ip-172-30-15-5.
+
....

[oracle@ip-172-30-15-5 odm]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Sep 12 18:30:04 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> startup;
ORACLE instance started.

Total System Global Area 6442449688 bytes
Fixed Size                  9177880 bytes
Variable Size            1090519040 bytes
Database Buffers         5335154688 bytes
Redo Buffers                7598080 bytes
Database mounted.
Database opened.
SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
NTAP      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 NTAP_PDB1                      READ WRITE NO
         4 NTAP_PDB2                      READ WRITE NO
         5 NTAP_PDB3                      READ WRITE NO


....
. db- und Failback-Datenbank auf den primären Knoten ip-172-30-15-111 herunterfahren.
+
....

SQL> shutdown immediate;
Database closed.
Database dismounted.
ORACLE instance shut down.
SQL> exit

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03

[root@ip-172-30-15-5 ec2-user]# umount /u01
[root@ip-172-30-15-5 ec2-user]# umount /u02
[root@ip-172-30-15-5 ec2-user]# umount /u03

[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u01 /u01 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u02 /u02 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# mount -t nfs 172.30.15.95:/orapm01_u03 /u03 -o rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536
mount: (hint) your fstab has been modified, but systemd still uses
       the old version; use 'systemctl daemon-reload' to reload.
[root@ip-172-30-15-111 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.8G   48M  7.7G   1% /dev/shm
tmpfs                      7.8G   33M  7.7G   1% /run
tmpfs                      7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/xvda2                  50G   29G   22G  58% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  844G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  844G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  844G 100% /u03
[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Thu Sep 12 18:13:34 UTC 2024 on pts/1
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Thu Sep 12 18:38:46 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Connected to an idle instance.

SQL> startup;
ORACLE instance started.

Total System Global Area 6442449688 bytes
Fixed Size                  9177880 bytes
Variable Size            1090519040 bytes
Database Buffers         5335154688 bytes
Redo Buffers                7598080 bytes
Database mounted.
Database opened.
SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0
[oracle@ip-172-30-15-111 ~]$ lsnrctl start listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 12-SEP-2024 18:39:17

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Starting /u01/app/oracle/product/19.0.0/NTAP/bin/tnslsnr: please wait...

TNSLSNR for Linux: Version 19.0.0.0.0 - Production
System parameter file is /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Log messages written to /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                12-SEP-2024 18:39:17
Uptime                    0 days 0 hr. 0 min. 0 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ip-172-30-15-111.ec2.internal)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
The listener supports no services
The command completed successfully

....


====


=== Konfigurieren Sie Oracle-Ressourcen für die PC-Verwaltung

[%collapsible]
====
Ziel der Konfiguration von Pacemaker Clustering ist die Einrichtung einer aktiv/Passiv-Hochverfügbarkeitslösung, mit der Oracle in AWS EC2- und FSX ONTAP-Umgebungen ausgeführt werden kann, wobei bei einem Ausfall nur minimale Eingriffe durch den Benutzer erforderlich sind. Im Folgenden wird die Konfiguration von Oracle-Ressourcen für die PC-Verwaltung beschrieben.

. Erstellen Sie als Root-Benutzer auf der primären EC2-Instanz ip-172-30-15-111 eine sekundäre private IP-Adresse mit einer nicht verwendeten privaten IP-Adresse im VPC CIDR-Block als fließende IP. Erstellen Sie dabei eine oracle-Ressourcengruppe, zu der die sekundäre private IP-Adresse gehört.
+
[source, cli]
----
pcs resource create privip ocf:heartbeat:awsvip secondary_private_ip=172.30.15.33 --group oracle
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:25:35 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:25:23 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 2 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-5.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
+

NOTE: Wenn der Privip auf dem Standby-Cluster-Node erstellt wird, verschieben Sie ihn wie unten gezeigt auf den primären Node.

. Verschieben Sie eine Ressource zwischen Clusterknoten.
+
[source, cli]
----
pcs resource move privip ip-172-30-15-111.ec2.internal
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs resource move privip ip-172-30-15-111.ec2.internal
Warning: A move constraint has been created and the resource 'privip' may or may not move depending on other configuration
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
Following resources have been moved and their move constraints are still in place: 'privip'
Run 'pcs constraint location' or 'pcs resource clear <resource id>' to view or remove the constraints, respectively

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:26:38 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:26:27 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 2 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal (Monitoring)

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. Erstellen Sie eine virtuelle IP (vip) für Oracle. Virtuelle IP-Daten werden bei Bedarf zwischen dem primären und dem Standby-Node float.
+
[source, cli]
----
pcs resource create vip ocf:heartbeat:IPaddr2 ip=172.30.15.33 cidr_netmask=25 nic=eth0 op monitor interval=10s --group oracle
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs resource create vip ocf:heartbeat:IPaddr2 ip=172.30.15.33 cidr_netmask=25 nic=eth0 op monitor interval=10s --group oracle
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx

WARNINGS:
Following resources have been moved and their move constraints are still in place: 'privip'
Run 'pcs constraint location' or 'pcs resource clear <resource id>' to view or remove the constraints, respectively

Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 16:27:34 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 16:27:24 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 3 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

....
. Aktualisieren Sie als oracle-Benutzer die Datei Listener.ora und tnsnames.ora, um auf die vip-Adresse zu verweisen. Starten Sie den Listener neu. Bounce-Datenbank, falls dies für die DB erforderlich ist, um sich beim Listener zu registrieren.
+
[source, cli]
----
vi $ORACLE_HOME/network/admin/listener.ora
----
+
[source, cli]
----
vi $ORACLE_HOME/network/admin/tnsnames.ora
----
+
....

[oracle@ip-172-30-15-111 admin]$ cat listener.ora
# listener.ora Network Configuration File: /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
# Generated by Oracle configuration tools.

LISTENER.NTAP =
  (DESCRIPTION_LIST =
    (DESCRIPTION =
      (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))
      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))
    )
  )

[oracle@ip-172-30-15-111 admin]$ cat tnsnames.ora
# tnsnames.ora Network Configuration File: /u01/app/oracle/product/19.0.0/NTAP/network/admin/tnsnames.ora
# Generated by Oracle configuration tools.

NTAP =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = NTAP.ec2.internal)
    )
  )

LISTENER_NTAP =
  (ADDRESS = (PROTOCOL = TCP)(HOST = 172.30.15.33)(PORT = 1521))


[oracle@ip-172-30-15-111 admin]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 13-SEP-2024 18:28:17

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                13-SEP-2024 18:15:51
Uptime                    0 days 0 hr. 12 min. 25 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=172.30.15.33)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ip-172-30-15-111.ec2.internal)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP/admin/NTAP/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "21f0b5cc1fa290e2e0636f0f1eacfd43.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b74445329119e0636f0f1eacec03.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b83929709164e0636f0f1eacacc3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAP.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAPXDB.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb1.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb2.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
The command completed successfully

**Oracle listener now listens on vip for database connection**
....
. Fügen Sie /u01, /u02, /u03 Mount-Punkte zur oracle-Ressourcengruppe hinzu.
+
[source, cli]
----
pcs resource create u01 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u01' directory='/u01' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
+
[source, cli]
----
pcs resource create u02 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u02' directory='/u02' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
+
[source, cli]
----
pcs resource create u03 ocf:heartbeat:Filesystem device='172.30.15.95:/orapm01_u03' directory='/u03' fstype='nfs' options='rw,bg,hard,vers=3,proto=tcp,timeo=600,rsize=65536,wsize=65536' --group oracle
----
. Erstellen Sie eine Benutzer-ID zur Überwachung von PCS in der oracle DB.
+
....

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Fri Sep 13 18:12:24 UTC 2024 on pts/0
[oracle@ip-172-30-15-111 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 19:08:41 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> CREATE USER c##ocfmon IDENTIFIED BY "XXXXXXXX";

User created.

SQL> grant connect to c##ocfmon;

Grant succeeded.

SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

....
. Datenbank zur oracle-Ressourcengruppe hinzufügen.
+
[source, cli]
----
pcs resource create ntap ocf:heartbeat:oracle sid='NTAP' home='/u01/app/oracle/product/19.0.0/NTAP' user='oracle' monuser='C##OCFMON' monpassword='XXXXXXXX' monprofile='DEFAULT' --group oracle
----
. Fügen Sie der oracle-Ressourcengruppe einen Datenbanklistener hinzu.
+
[source, cli]
----
pcs resource create listener ocf:heartbeat:oralsnr sid='NTAP' listener='listener.ntap' --group=oracle
----
. Aktualisieren Sie alle Einschränkungen für den Ressourcenspeicherort in der oracle-Ressourcengruppe auf den primären Knoten als bevorzugten Knoten.
+
[source, cli]
----
pcs constraint location privip prefers ip-172-30-15-111.ec2.internal
pcs constraint location vip prefers ip-172-30-15-111.ec2.internal
pcs constraint location u01 prefers ip-172-30-15-111.ec2.internal
pcs constraint location u02 prefers ip-172-30-15-111.ec2.internal
pcs constraint location u03 prefers ip-172-30-15-111.ec2.internal
pcs constraint location ntap prefers ip-172-30-15-111.ec2.internal
pcs constraint location listener prefers ip-172-30-15-111.ec2.internal
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs constraint config
Location Constraints:
  Resource: listener
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: ntap
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: privip
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u01
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u02
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: u03
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
  Resource: vip
    Enabled on:
      Node: ip-172-30-15-111.ec2.internal (score:INFINITY)
Ordering Constraints:
Colocation Constraints:
Ticket Constraints:

....
. Validierung der Konfiguration von Oracle-Ressourcen
+
[source, cli]
----
pcs status
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 19:25:32 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 19:23:40 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Started ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled


....


====


=== HA-Validierung nach der Implementierung

[%collapsible]
====
Nach der Bereitstellung müssen einige Tests und Validierungen durchgeführt werden, um sicherzustellen, dass der Failover-Cluster der PCS Oracle-Datenbank korrekt konfiguriert ist und wie erwartet funktioniert. Die Testvalidierung umfasst Managed Failover und simulierte unerwartete Ressourcenausfälle und Recovery durch den Cluster-Schutzmechanismus.

. Validierung des Node-Fencing durch manuelles Auslösen der Fencing des Standby-Node und Beobachtung, dass der Standby-Node nach einer Zeitüberschreitung offline geschaltet und neu gestartet wurde.
+
[source, cli]
----
pcs stonith fence <standbynodename>
----
+
....

[root@ip-172-30-15-111 ec2-user]# pcs stonith fence ip-172-30-15-5.ec2.internal
Node: ip-172-30-15-5.ec2.internal fenced
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 21:58:45 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 21:55:12 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-111.ec2.internal ]
  * OFFLINE: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-111.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Started ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Started ip-172-30-15-111.ec2.internal

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled


....
. Simulieren Sie einen Ausfall eines Datenbanklisteners, indem Sie den Listener-Prozess abtöten und beobachten, dass PCS den Ausfall des Listeners überwachen und ihn in wenigen Sekunden neu starten.
+
....

[root@ip-172-30-15-111 ec2-user]# ps -ef | grep lsnr
oracle    154895       1  0 18:15 ?        00:00:00 /u01/app/oracle/product/19.0.0/NTAP/bin/tnslsnr listener.ntap -inherit
root      217779  120186  0 19:36 pts/0    00:00:00 grep --color=auto lsnr
[root@ip-172-30-15-111 ec2-user]# kill -9 154895

[root@ip-172-30-15-111 ec2-user]# su - oracle
Last login: Thu Sep 19 14:58:54 UTC 2024
[oracle@ip-172-30-15-111 ~]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 13-SEP-2024 19:36:51

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused
Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=IPC)(KEY=EXTPROC1521)))
TNS-12541: TNS:no listener
 TNS-12560: TNS:protocol adapter error
  TNS-00511: No listener
   Linux Error: 111: Connection refused

[oracle@ip-172-30-15-111 ~]$ lsnrctl status listener.ntap

LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 19-SEP-2024 15:00:10

Copyright (c) 1991, 2022, Oracle.  All rights reserved.

Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=172.30.15.33)(PORT=1521)))
STATUS of the LISTENER
------------------------
Alias                     listener.ntap
Version                   TNSLSNR for Linux: Version 19.0.0.0.0 - Production
Start Date                16-SEP-2024 14:00:14
Uptime                    3 days 0 hr. 59 min. 56 sec
Trace Level               off
Security                  ON: Local OS Authentication
SNMP                      OFF
Listener Parameter File   /u01/app/oracle/product/19.0.0/NTAP/network/admin/listener.ora
Listener Log File         /u01/app/oracle/diag/tnslsnr/ip-172-30-15-111/listener.ntap/alert/log.xml
Listening Endpoints Summary...
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=172.30.15.33)(PORT=1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=ipc)(KEY=EXTPROC1521)))
  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ip-172-30-15-111.ec2.internal)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/product/19.0.0/NTAP/admin/NTAP/xdb_wallet))(Presentation=HTTP)(Session=RAW))
Services Summary...
Service "21f0b5cc1fa290e2e0636f0f1eacfd43.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b74445329119e0636f0f1eacec03.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "21f0b83929709164e0636f0f1eacacc3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAP.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "NTAPXDB.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb1.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb2.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
Service "ntap_pdb3.ec2.internal" has 1 instance(s).
  Instance "NTAP", status READY, has 1 handler(s) for this service...
The command completed successfully

....
. Simulieren Sie einen Datenbankfehler, indem Sie den pmon-Prozess abtöten und beobachten, dass PCS den Datenbankfehler überwachen und ihn in wenigen Sekunden neu starten.
+
....

**Make a remote connection to ntap database**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 15:42:42 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Thu Sep 12 2024 13:37:28 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>

**Kill ntap pmon process to simulate a failure**

[root@ip-172-30-15-111 ec2-user]# ps -ef | grep pmon
oracle    159247       1  0 18:27 ?        00:00:00 ora_pmon_NTAP
root      230595  120186  0 19:44 pts/0    00:00:00 grep --color=auto pmon
[root@ip-172-30-15-111 ec2-user]# kill -9 159247

**Observe the DB failure**

SQL> /
select instance_name, host_name from v$instance
*
ERROR at line 1:
ORA-03113: end-of-file on communication channel
Process ID: 227424
Session ID: 396 Serial number: 4913


SQL> exit
Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

**Reconnect to DB after reboot**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 15:47:24 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 15:42:47 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>


....
. Validierung eines verwalteten Datenbank-Failover vom primären zum Standby-Modus durch Verschiebung des primären Node in den Standby-Modus zum Failover von Oracle-Ressourcen auf den Standby-Node
+
[source, cli]
----
pcs node standby <nodename>
----
+
....

**Stopping Oracle resources on primary node in reverse order**

[root@ip-172-30-15-111 ec2-user]# pcs node standby ip-172-30-15-111.ec2.internal
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:01:16 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:01:08 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Node ip-172-30-15-111.ec2.internal: standby (with active resources)
  * Online: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Stopping ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Stopped
    * u03       (ocf::heartbeat:Filesystem):     Stopped
    * ntap      (ocf::heartbeat:oracle):         Stopped
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Starting Oracle resources on standby node in sequencial order**

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:01:34 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:01:08 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Node ip-172-30-15-111.ec2.internal: standby
  * Online: [ ip-172-30-15-5.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-5.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-5.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-5.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Starting ip-172-30-15-5.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**NFS mount points mounted on standby node**

[root@ip-172-30-15-5 ec2-user]# df -h
Filesystem                 Size  Used Avail Use% Mounted on
devtmpfs                   7.7G     0  7.7G   0% /dev
tmpfs                      7.7G   33M  7.7G   1% /dev/shm
tmpfs                      7.7G   17M  7.7G   1% /run
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
/dev/xvda2                  50G   21G   30G  41% /
tmpfs                      1.6G     0  1.6G   0% /run/user/1000
172.30.15.95:/orapm01_u01   48T   47T  840G  99% /u01
172.30.15.95:/orapm01_u02  285T  285T  840G 100% /u02
172.30.15.95:/orapm01_u03  190T  190T  840G 100% /u03
tmpfs                      1.6G     0  1.6G   0% /run/user/54321

**Database opened on standby node**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 16:34:08 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 15:47:28 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
NTAP      READ WRITE

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-5.ec2.internal


SQL>

....
. Validieren Sie ein verwaltetes Datenbank-Failback vom Standby zum primären Node durch den Standby-primären Node und beobachten Sie, dass Oracle-Ressourcen aufgrund der bevorzugten Node-Einstellung automatisch ein Failback durchführen.
+
[source, cli]
----
pcs node unstandby <nodename>
----
+
....
**Stopping Oracle resources on standby node for failback to primary**

[root@ip-172-30-15-111 ec2-user]# pcs node unstandby ip-172-30-15-111.ec2.internal
[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:41:30 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:41:18 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Stopping ip-172-30-15-5.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Stopped
    * u01       (ocf::heartbeat:Filesystem):     Stopped
    * u02       (ocf::heartbeat:Filesystem):     Stopped
    * u03       (ocf::heartbeat:Filesystem):     Stopped
    * ntap      (ocf::heartbeat:oracle):         Stopped
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Starting Oracle resources on primary node for failback**

[root@ip-172-30-15-111 ec2-user]# pcs status
Cluster name: ora_ec2nfsx
Cluster Summary:
  * Stack: corosync (Pacemaker is running)
  * Current DC: ip-172-30-15-111.ec2.internal (version 2.1.7-5.1.el8_10-0f7f88312) - partition with quorum
  * Last updated: Fri Sep 13 20:41:45 2024 on ip-172-30-15-111.ec2.internal
  * Last change:  Fri Sep 13 20:41:18 2024 by root via root on ip-172-30-15-111.ec2.internal
  * 2 nodes configured
  * 8 resource instances configured

Node List:
  * Online: [ ip-172-30-15-5.ec2.internal ip-172-30-15-111.ec2.internal ]

Full List of Resources:
  * clusterfence        (stonith:fence_aws):     Started ip-172-30-15-5.ec2.internal
  * Resource Group: oracle:
    * privip    (ocf::heartbeat:awsvip):         Started ip-172-30-15-111.ec2.internal
    * vip       (ocf::heartbeat:IPaddr2):        Started ip-172-30-15-111.ec2.internal
    * u01       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u02       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * u03       (ocf::heartbeat:Filesystem):     Started ip-172-30-15-111.ec2.internal
    * ntap      (ocf::heartbeat:oracle):         Starting ip-172-30-15-111.ec2.internal
    * listener  (ocf::heartbeat:oralsnr):        Stopped

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

**Database now accepts connection on primary node**

[oracle@ora_01 ~]$ sqlplus system@//172.30.15.33:1521/NTAP.ec2.internal

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Sep 13 16:46:07 2024
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.

Enter password:
Last Successful login time: Fri Sep 13 2024 16:34:12 -04:00

Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select instance_name, host_name from v$instance;

INSTANCE_NAME
----------------
HOST_NAME
----------------------------------------------------------------
NTAP
ip-172-30-15-111.ec2.internal


SQL>

....


Hiermit sind die Oracle HA-Validierung und Lösungsvorführung in AWS EC2 mit Pacemaker-Clustering und Amazon FSX ONTAP als Datenbank-Storage-Backend abgeschlossen.

====


=== Backup, Wiederherstellung und Klonen von Oracle mit SnapCenter

[%collapsible]
====
NetApp empfiehlt das SnapCenter UI-Tool für das Management der in AWS EC2 und Amazon FSX ONTAP implementierten Oracle Datenbank. Im link:aws_ora_fsx_vmc_guestmount.html#oracle-backup-restore-and-clone-with-snapcenter["Vereinfachtes, selbstverwaltetes Oracle in VMware Cloud on AWS mit Gast-Mounted FSX ONTAP"^]Abschnitt TR-4979  `Oracle backup, restore, and clone with SnapCenter`finden Sie Details zur Einrichtung von SnapCenter und zur Ausführung von Datenbank-Backup-, Restore- und Klon-Workflows.

====


== Wo Sie weitere Informationen finden

Weitere Informationen zu den in diesem Dokument beschriebenen Daten finden Sie in den folgenden Dokumenten bzw. auf den folgenden Websites:

* Konfiguration und Management von Hochverfügbarkeits-Clustern
+
link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index["https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/index"^]

* NetApp Lösungen für Enterprise Database
+
link:https://docs.netapp.com/us-en/netapp-solutions/databases/index.html["https://docs.netapp.com/us-en/netapp-solutions/databases/index.html"^]

* Amazon FSX für NetApp ONTAP
+
link:https://aws.amazon.com/fsx/netapp-ontap/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d["https://aws.amazon.com/fsx/netapp-ontap/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d"^]

* Bereitstellung von Oracle Direct NFS
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/deploying-dnfs.html#GUID-D06079DB-8C71-4F68-A1E3-A75D7D96DCE2"^]


