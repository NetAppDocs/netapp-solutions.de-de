---
sidebar: sidebar 
permalink: data-analytics/apache-spark-solution-technology.html 
keywords: standalone, apache mesos, hadoop yarn, resilient distributed dataset, rdd, dataframe, hadoop distributed file system, hdfs 
summary: In diesem Abschnitt werden die Art und die Komponenten von Apache Spark sowie deren Beitrag zu dieser Lösung beschrieben. 
---
= Lösungstechnologie
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
Apache Spark ist ein beliebtes Programmierungs-Framework für Hadoop Applikationen, das direkt mit Hadoop Distributed File System (HDFS) zusammenarbeitet. Spark ist produktionsbereit, unterstützt die Verarbeitung von Streaming-Daten und ist schneller als MapReduce. Spark verfügt über ein in-Memory-Daten-Caching und sorgt so für eine effiziente Iteration. Die Spark Shell ist interaktiv für das Lernen und die Erforschung von Daten. Mit Spark können Sie Anwendungen in Python, Scala oder Java erstellen. Funkenanwendungen bestehen aus einem oder mehreren Jobs, die eine oder mehrere Aufgaben haben.

Jede Spark-Anwendung verfügt über einen Spark-Treiber. Im YARN-Client-Modus wird der Treiber lokal auf dem Client ausgeführt. Im YARN-Cluster-Modus wird der Treiber im Cluster auf dem Anwendungsmaster ausgeführt. Im Cluster-Modus wird die Applikation weiterhin ausgeführt, auch wenn die Verbindung des Clients getrennt wird.

image:apache-spark-image3.png["Fehler: Fehlendes Grafikbild"]

Es gibt drei Cluster Manager:

* *Standalone.* dieser Manager ist ein Teil von Spark, was es einfach macht, einen Cluster einzurichten.
* *Apache Mesos.* Dies ist ein allgemeiner Clustermanager, der auch MapReduce und andere Anwendungen ausführt.
* *Hadoop YARN.* Dies ist ein Resource Manager in Hadoop 3.


Der Resilient Distributed Dataset (RDD) ist die primäre Komponente von Spark. RDD erstellt die verlorenen und fehlenden Daten aus dem Speicher des Clusters neu und speichert die ursprünglichen Daten, die aus einer Datei stammen oder programmgesteuert erstellt werden. RDDs werden aus Dateien, Daten im Speicher oder einem anderen RDD erstellt. Die Funkenprogrammierung führt zwei Vorgänge durch: Transformation und Aktionen. Durch Transformation wird ein neues RDD auf Basis eines vorhandenen erstellt. Aktionen geben einen Wert aus einem RDD zurück.

Transformationen und Aktionen gelten auch für Spark DataFrames und DataFrames. Ein Datensatz ist eine verteilte Sammlung von Daten, die die Vorteile von RDDs (starke Eingabe, Verwendung von Lambda-Funktionen) mit den Vorteilen der optimierten Ausführung von Spark SQL bietet. Ein Datensatz kann aus JVM-Objekten erstellt und anschließend mithilfe von funktionalen Transformationen (Map, FlatMap, Filter usw.) manipuliert werden. Ein DataFrame ist ein Datensatz, der in benannte Spalten organisiert ist. Es ist konzeptionell gleichbedeutend mit einer Tabelle in einer relationalen Datenbank oder einem Datenrahmen in R/Python. Datenframes können aus einer Vielzahl von Quellen wie strukturierten Datendateien, Tabellen in Hive/HBase, externen Datenbanken vor Ort oder in der Cloud oder vorhandenen RDDs erstellt werden.

Funkenanwendungen umfassen einen oder mehrere Spark-Jobs. Die Jobs führen Aufgaben in Ausführenden aus, und die Ausführenden werden in YARN-Containern ausgeführt. Jeder Ausführende wird in einem einzigen Container ausgeführt und Ausführende existieren während des gesamten Lebenszyklus einer Applikation. Ein Executor wird nach dem Start der Anwendung repariert und YARN ändert nicht die Größe des bereits zugewiesenen Containers. Ein Ausführender kann Aufgaben gleichzeitig auf Speicherdaten ausführen.
