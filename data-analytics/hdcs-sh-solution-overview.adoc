---
sidebar: sidebar 
permalink: data-analytics/hdcs-sh-solution-overview.html 
keywords: tr-4657, tr4657, 4657, hybrid cloud, spark, hadoop, aff, fas 
summary: In diesem Dokument werden Hybrid-Cloud-Datenlösungen beschrieben, die NetApp AFF und FAS Storage-Systeme, NetApp Cloud Volumes ONTAP, NetApp Connected Storage sowie NetApp FlexClone Technologie für Spark und Hadoop einsetzen. Mit diesen Lösungsarchitekturen können Kunden eine geeignete Datensicherungslösung für ihre Umgebung wählen. NetApp hat diese Lösungen basierend auf der Interaktion mit Kunden und eigenen Business-Anwendungsfällen entwickelt. 
---
= TR-4657: NetApp Hybrid-Cloud-Datenlösungen – Spark und Hadoop auf Basis von Kundenanwendungsfällen
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


Karthikeyan Nagalingam und Sathish Thyagarajan, NetApp

[role="lead"]
In diesem Dokument werden Hybrid-Cloud-Datenlösungen beschrieben, die NetApp AFF und FAS Storage-Systeme, NetApp Cloud Volumes ONTAP, NetApp Connected Storage sowie NetApp FlexClone Technologie für Spark und Hadoop einsetzen. Mit diesen Lösungsarchitekturen können Kunden eine geeignete Datensicherungslösung für ihre Umgebung wählen. NetApp hat diese Lösungen basierend auf der Interaktion mit Kunden und eigenen Business-Anwendungsfällen entwickelt. Dieses Dokument enthält folgende detaillierte Informationen:

* Warum wir Datensicherung für Spark und Hadoop-Umgebungen sowie auf Kundenherausforderungen benötigen
* Die Data Fabric basiert auf der NetApp Vision und den Bausteinen und Services.
* Wie diese Bausteine zur Erstellung flexibler Datensicherungs-Workflows eingesetzt werden können
* Vor- und Nachteile verschiedener Architekturen, die auf realistischen Kundenanwendungsfällen basieren. Jeder Anwendungsfall umfasst die folgenden Komponenten:
+
** Kundenszenarien
** Anforderungen und Herausforderungen zu bewältigen
** Verbessern Lassen
** Zusammenfassung der Lösungen






== Warum Hadoop Datensicherung?

In einer Hadoop und Spark-Umgebung müssen die folgenden Aspekte berücksichtigt werden:

* *Software oder menschliches Versagen.* menschliches Versagen bei Software-Updates während der Durchführung von Hadoop-Datenoperationen kann zu fehlerhaftem Verhalten führen, das unerwartete Ergebnisse aus dem Job verursachen kann. In diesem Fall müssen wir die Daten schützen, um Fehler oder unangemessene Ergebnisse zu vermeiden. Als Ergebnis eines schlecht ausgeführten Softwareupdates auf eine Traffic-Signal-Analyse-Anwendung, eine neue Funktion, die nicht richtig Analyse von Verkehrssignaldaten in Form von Klartext. Die Software analysiert weiterhin JSON und andere nicht-Text-Dateiformate, was zu einem Echtzeit-System zur Verkehrssteuerung führt, in dem Vorhersageergebnisse vorliegen, die keine Datenpunkte enthalten. Dies kann zu fehlerhaften Ausgängen führen, die zu Unfällen an den Verkehrssignalen führen können. Datensicherheit kann dieses Problem lösen, indem sie die Möglichkeit bietet, schnell ein Rollback zur vorherigen Applikationsversion durchführen zu können.
* *Größe und Umfang.* die Größe der Analysedaten wächst Tag für Tag aufgrund der stetig wachsenden Zahl von Datenquellen und Volumen. Social Media, mobile Apps, Data Analytics und Cloud-Computing-Plattformen sind die wichtigsten Datenquellen des aktuellen Big Data-Marktes, der sehr schnell zunimmt und daher die Daten zur Sicherstellung akkurater Datenabläufe geschützt werden müssen.
* *Der native Datenschutz von Hadoop.* Hadoop hat einen nativen Befehl, um die Daten zu schützen, aber dieser Befehl bietet keine Konsistenz der Daten während des Backups. Es unterstützt nur Backups auf Verzeichnisebene. Die von Hadoop erstellten Snapshots sind schreibgeschützt und können nicht zur direkten Wiederverwendung der Backup-Daten verwendet werden.




== Herausforderungen bei der Datensicherung für Hadoop und Spark Kunden

Kunden in Hadoop und Spark stehen vor der Herausforderung, die Backup-Zeit zu reduzieren und die Backup-Zuverlässigkeit zu steigern, ohne während der Datensicherung die Performance im Produktions-Cluster zu beeinträchtigen.

Kunden müssen außerdem Ausfallzeiten bei Recovery Point Objective (RPO) und Recovery Time Objective (RTO) minimieren und ihre Disaster-Recovery-Standorte vor Ort und in der Cloud kontrollieren, um optimale Business Continuity zu erzielen. Diese Kontrollfunktion ergibt sich in der Regel aus Management Tools der Enterprise-Klasse.

Die Umgebungen mit Hadoop und Spark sind kompliziert, da nicht nur das Datenvolumen sehr groß ist und immer größer wird, sondern auch die Geschwindigkeit, mit der die Daten ankommen. Bei diesem Szenario ist es schwierig, schnell effiziente, aktuelle DevTest- und QA-Umgebungen aus den Quelldaten zu erstellen. NetApp kennt diese Herausforderungen und bietet die in diesem Whitepaper vorgestellten Lösungen.

link:hdcs-sh-data-fabric-powered-by-netapp-for-big-data-architecture.html["Als Nächstes: Data Fabric von NetApp für Big-Data-Architekturen"]
