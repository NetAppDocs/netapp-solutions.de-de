---
sidebar: sidebar 
permalink: data-analytics/apache-spark-major-ai-ml-and-dl-use-cases-and-architectures.html 
keywords: nlp pipelines, tensorflow distributed inferenceing, horovod distributed training, multi-worker, deep learning, keras, ctr prediction 
summary: Auf dieser Seite werden die wichtigsten Anwendungsfälle und Architekturen für KI, ML und DL im Detail beschrieben. 
---
= Anwendungsfälle und Architekturen für KI, ML und DL
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Größere Anwendungsfälle und Methoden für KI, ML und DL können in die folgenden Abschnitte unterteilt werden:



== Spark NLP-Pipelines und TensorFlow Distributed Inferenzierung

Die folgende Liste enthält die beliebtesten Open-Source-NLP-Bibliotheken, die von der Data Science-Community unter verschiedenen Entwicklungsstufen übernommen wurden:

* https://www.nltk.org/["Natural Language Toolkit (NLTK)"^]. Das komplette Toolkit für alle NLP-Techniken. Es wurde seit den frühen 2000er Jahren erhalten.
* https://textblob.readthedocs.io/en/dev/["TextBlob"^]. Eine einfach zu bedienende NLP-Tools Python-API, die auf NLTK und Pattern aufgebaut ist.
* https://stanfordnlp.github.io/CoreNLP/["Stanford Core NLP"^]. NLP-Dienste und -Pakete in Java, entwickelt von der Stanford NLP Group.
* https://radimrehurek.com/gensim/["Gensim"^]. Thema Modellierung für Menschen begann als Sammlung von Python-Skripten für das Projekt der Tschechischen Digital Mathematics Library.
* https://spacy.io/["Spacy"^]. Durchgängige industrielle NLP-Workflows mit Python und Cython mit GPU-Beschleunigung für Transformatoren.
* https://fasttext.cc/["Fasttext"^]. Eine kostenlose, leichte, Open-Source-NLP-Bibliothek für das Lernen von Wortschaltungen und Satzklassifizierung, die im AI Research Lab (FAIR) von Facebook erstellt wurde.


Spark NLP ist eine einzige, einheitliche Lösung für alle NLP-Aufgaben und -Anforderungen, die skalierbare, leistungsstarke und hochpräzise NLP-gestützte Software für reale Produktionsanwendungsfälle ermöglicht. Es nutzt Transfer-Learning und implementiert modernste Algorithmen und Modelle in der Forschung und Industrie. Da Spark keine volle Unterstützung für die oben genannten Bibliotheken hat, wurde Spark NLP auf der Basis aufgebaut https://spark.apache.org/docs/latest/ml-guide.html["Funken ML"^] Um die Vorteile der universellen in-Memory Distributed Data Processing Engine von Spark als NLP-Bibliothek der Enterprise-Klasse für unternehmenskritische Produktions-Workflows zu nutzen. Die Kommentatoren nutzen regelbasierte Algorithmen, Machine Learning und TensorFlow, um Deep-Learning-Implementierungen zu unterstützen. Dies betrifft gängige NLP-Aufgaben, einschließlich aber nicht beschränkt auf Tokenisierung, Lemmatisierung, Stemming, Teil-of-Speech-Tagging, benannte Entity-Erkennung, Rechtschreibprüfung und Sentiment-Analyse.

Bidirektionale Encoder-Darstellungen von Transformatoren (BERT) ist eine Transformatorbasierte Machine Learning-Technik für NLP. Es popularisierte das Konzept der Vorausbildung und Feinabstimmung. Die Transformatorarchitektur in BERT entstand aus maschineller Übersetzung, die langfristige Abhängigkeiten besser modelt als rezidierende Neural Network (RNN)-basierte Sprachmodelle. Außerdem wurde die MLM-Aufgabe (Masked Language Modeling) eingeführt, bei der zufällige 15 % aller Token maskiert und das Modell sie vorhersagt, wodurch eine echte Bidirektionalität möglich ist.

Die Analyse der Finanzstimmung ist aufgrund der Fachsprache und des Mangels an gekennzeichneten Daten in diesem Bereich schwierig. https://nlp.johnsnowlabs.com/2021/11/03/bert_sequence_classifier_finbert_en.html["FinBERT"^], Ein Sprachmodell basierend auf vortrainierten BERT, wurde Domain angepasst https://trec.nist.gov/data/reuters/reuters.html["Reuters-TRC2"^], Ein Finanzkorpus, und fein abgestimmt mit beschrifteten Daten ( https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10["FinanzphraseBank"^]) Für die Klassifizierung der Finanzstimmung. Forscher haben 4, 500 Sätze aus Nachrichten-Artikeln mit finanziellen Begriffen extrahiert. Dann 16 Experten und Master Studenten mit Finanz-Hintergrund bezeichnet die Sätze als positiv, neutral und negativ. Wir haben einen End-to-End Spark-Workflow entwickelt, um die Stimmung für die von 2016 bis 2020 am NASDAQ am Markt vertelefonisch unter den Top-10-Unternehmen zu analysieren und FinBERT sowie zwei weitere vortrainierte Pipelines ( https://nlp.johnsnowlabs.com/2021/11/11/classifierdl_bertwiki_finance_sentiment_pipeline_en.html["Sentiment Analysis for Financial News"^], https://nlp.johnsnowlabs.com/2020/03/19/explain_document_dl.html["Dokument-DL erklären"^]) Von Spark NLP.

Die zugrunde liegende Deep-Learning-Engine für Spark NLP ist TensorFlow, eine End-to-End-Open-Source-Plattform für Machine Learning, die den einfachen Modellbau, die robuste ML-Produktion an jedem Ort und leistungsstarke Experimente ermöglicht. Deshalb bei der Ausführung unserer Pipelines in Spark `yarn cluster` Modus, wir hatten im Wesentlichen Distributed TensorFlow mit Daten und Modellparallelisierung über einen Master- und mehrere Worker-Node sowie über den auf dem Cluster angebundenen Network-Attached Storage.



== Horovod Distributed Training

Die Kernvalidierung für MapReduce-bezogene Performance wird mit TeraGen, TeraSort, TeraValidate und DFSIO (Lese- und Schreibvorgänge) durchgeführt. Die Ergebnisse der TeraGen und TeraSort Validierung werden in vorgestellt http://www.netapp.com/us/media/tr-3969.pdf["TR-3969: NetApp Lösungen für Hadoop"^] Für E-Series und im Abschnitt „Storage Tiering“ (xref) für AFF.

Auf Kundenwünsche basieren verteilte Schulungen mit Spark auf einer der wichtigsten Nutzungsfälle. In diesem Dokument wurden von verwendet https://horovod.readthedocs.io/en/stable/spark_include.html["Hovorod auf Spark"^] Spark-Performance mit NetApp On-Premises-, Cloud-nativen und Hybrid-Cloud-Lösungen mit NetApp All Flash FAS (AFF) Storage-Controllern, Azure NetApp Files und StorageGRID validieren

Das Paket Horovod on Spark bietet eine praktische Wrapper für Horovod, mit der verteilte Trainings-Workloads in Spark-Clustern einfach ausgeführt werden. Es ermöglicht einen eng aneinander auslaufenden Modelldesign-Loop, in dem Datenverarbeitung, Modelltraining und Modellevaluierung in Spark ausgeführt werden, wo sich die Trainings- und Inferenzdaten befinden.

Es gibt zwei APIs für die Ausführung von Horovod auf Spark: Eine High-Level Estimator API und eine Low-Level-Run-API. Obwohl beide den gleichen zugrunde liegenden Mechanismus verwenden, um Horovod auf Spark-Executoren zu starten, abstrahiert die Estimator-API die Datenverarbeitung, Modelltrainingschleife, Modell Checkpointing, Kennzahlenerfassung und verteilte Schulung. Wir haben Horovod Spark Estimators, TensorFlow und Keras eingesetzt, um eine End-to-End-Datenvorbereitung und einen verteilten Trainings-Workflow auf Basis der zu ermöglichen https://www.kaggle.com/c/rossmann-store-sales["Kaggle Rossmann Store Sales"^] Die Wettbewerbssituation.

Das Skript `keras_spark_horovod_rossmann_estimator.py` Finden Sie im Abschnitt link:apache-spark-python-scripts-for-each-major-use-case.html["Python-Skripte für jeden größeren Anwendungsfall."] Sie besteht aus drei Teilen:

* Der erste Teil führt verschiedene Schritte zur Datenvorverarbeitung über einen ersten Satz von CSV-Dateien durch, die von Kaggle bereitgestellt und von der Community gesammelt werden. Die Eingabedaten werden mit einem in ein Trainingssatz unterteilt `Validation` Teilmenge und ein Testdatensatz.
* Der zweite Teil definiert ein Keras Deep Neural Network (DNN)-Modell mit logarithmischer Sigmoid-Aktivierungsfunktion und einem Adam-Optimizer und führt Distributed Training des Modells mit Horovod on Spark durch.
* Der dritte Teil führt eine Prognose für den Testdatensatz durch. Dabei wird das beste Modell verwendet, durch das der Validierungssatz insgesamt für absolute Fehler minimiert wird. Anschließend wird eine CSV-Ausgabedatei erstellt.


Siehe Abschnitt link:apache-spark-use-cases-summary.html#machine-learning["„Maschinelles Lernen“"] Für verschiedene Laufzeitvergleichsergebnisse.



== Deep Learning mit mehreren Mitarbeitern und Keras für die CTR-Vorhersage

Aufgrund der jüngsten Fortschritte BEI ML-Plattformen und -Applikationen wird jetzt viel Aufmerksamkeit auf das Lernen in großen Umgebungen gerichtet. Die Klickrate (Klickrate, CTR) ist definiert als die durchschnittliche Anzahl der Klickrate pro hundert Online-Werbeeindrücke (ausgedrückt als Prozentsatz). Es wird in verschiedenen Branchen und Anwendungsfällen, darunter digitales Marketing, Einzelhandel, E-Commerce und Service-Provider, häufig als wichtige Metrik eingeführt. Sehen Sie unsere link:../ai/aks-anf_introduction.html["TR-4904: Distributed Training in Azure - Click-Through Rate Prediction"^] Weitere Details zu den Applikationen von CTR und einer End-to-End-Cloud-KI-Workflow-Implementierung mit Kubernetes, Distributed Data ETL und Modelltraining mit Dask und CUDA ML

In diesem technischen Bericht haben wir eine Variation der verwendet https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/["Criteo Terabyte Klicken Sie auf Protokolldatensatz"^] (Siehe TR-4904) für verteiltes Deep Learning mit Keras für mehrere Mitarbeiter, um einen Spark-Workflow mit Deep and Cross Network (DCN) Modellen zu erstellen, und vergleicht seine Leistung hinsichtlich der Log-Loss-Fehlerfunktion mit einem Basismodell des Spark ML Logistic Regression. DCN erfasst effiziente Interaktionen von Funktionen in begrenzt abstufenden Graden, erlernt hochnichtlineare Interaktionen, erfordert keine manuelle Funktionstechnik oder umfassende Suche und hat geringe Rechenkosten.

Daten für webbasierte Empfehlungssysteme sind meist diskret und kategorisch. Dies führt zu einem großen und spärlichen Speicherplatz, der für die Untersuchung von Funktionen eine Herausforderung darstellt. Dies hat die meisten Großsysteme auf lineare Modelle wie logistische Regression beschränkt. Das Erkennen häufig prädiktiver Funktionen und gleichzeitig das Erforschen von unsichtbaren oder seltenen Cross-Funktionen ist jedoch der Schlüssel für gute Vorhersagen. Lineare Modelle sind einfach, auswertbar und einfach zu skalieren, aber sie sind in ihrer Ausdruckskraft begrenzt.

Kreuzstücke hingegen haben sich als bedeutsam erwiesen, um die Ausdruckskraft der Modelle zu verbessern. Leider ist oftmals eine manuelle Funktionstechnik oder eine umfassende Suche erforderlich, um solche Funktionen zu identifizieren. Die Verallgemeinerung von Interaktionen mit unsichtbaren Funktionen ist oft schwierig. Die Verwendung eines neuronalen Netzwerks wie DCN vermeidet aufgabenspezifischen Funktionengineering, indem die Funktionsübergänge explizit automatisch angewendet werden. Das Cross-Netzwerk besteht aus mehreren Ebenen, wobei der höchste Grad an Interaktionen durch die Schichttiefe bestimmt wird. Jede Ebene erzeugt Interaktionen in höherer Reihenfolge, die auf bestehenden basieren, und behält die Interaktionen von vorherigen Ebenen.

Ein Deep Neural Network (DNN) verspricht sehr komplexe Interaktionen über verschiedene Funktionen hinweg. Im Vergleich zu DCN benötigt es jedoch fast eine Größenordnung von mehr Parametern, ist nicht in der Lage, Querfunktionen explizit zu bilden und kann möglicherweise nicht effizient lernen einige Arten von Feature-Interaktionen. Das Cross Network ist speichereffizient und einfach zu implementieren. Die gemeinsame Schulung der Cross- und DNN-Komponenten ermöglicht eine effiziente Erfassung prädiktiver Feature-Interaktionen und liefert hochmoderne Performance im Criteo CTR-Datensatz.

Ein DCN-Modell beginnt mit einer Einbettung- und Stapelschicht, gefolgt von einem Cross-Netzwerk und einem tiefen Netzwerk parallel. Auf diese wiederum folgt eine endgültige Kombinationsschicht, die die Ausgänge der beiden Netzwerke miteinander kombiniert. Ihre Eingabedaten können ein Vektor mit spärlichen und dichten Funktionen sein. In Spark, beide https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.linalg.SparseVector.html["Ml"^] Und https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.mllib.linalg.SparseVector.html["Mllib"^] Bibliotheken enthalten den Typ `SparseVector`. Daher ist es wichtig, dass die Benutzer zwischen den beiden unterscheiden und beim Aufruf ihrer jeweiligen Funktionen und Methoden achtsam sind. Bei empfohlenen Web-Scale-Systemen wie der CTR-Vorhersage handelt es sich beispielsweise um kategorische Merkmale `‘country=usa’`. Solche Funktionen werden oft als ein-Hot-Vektoren kodiert, z. B. `‘[0,1,0, …]’`. One-Hot-Encoding (OHE) mit `SparseVector` Ist nützlich beim Umgang mit Datensätzen aus der realen Welt mit sich ständig verändernden und wachsenden Vokabularen. Wir haben Beispiele in geändert https://github.com/shenweichen/DeepCTR["DeepCTR"^] Um große Vokabularblätter zu verarbeiten, erstellen Einbettungsvektoren in der Einbettung- und Stapelschicht unseres DCN.

Der https://www.kaggle.com/competitions/criteo-display-ad-challenge/data["Criteo Display Ads-Datensatz"^] Sagt die Durchklickrate für Werbeanzeigen aus. Es verfügt über 13 ganzzahlige Merkmale und 26 kategorische Merkmale, in denen jede Kategorie eine hohe Kardinalität hat. Bei diesem Datensatz ist eine Verbesserung von 0.001 im Logloss aufgrund der großen Eingangsgröße praktisch signifikant. Eine kleine Verbesserung der Vorhersagegenauigkeit für eine große Nutzerbasis kann möglicherweise zu einer großen Steigerung der Unternehmenseinnahmen führen. Der Datensatz enthält 11 GB Benutzerprotokolle von einem Zeitraum von 7 Tagen, was bedeutet etwa 41 Millionen Datensätzen. Wir haben Spark genutzt `dataFrame.randomSplit()function` Zur zufälligen Aufteilung der Daten für das Training (80 %), der Cross-Validierungen (10 %) und der verbleibenden 10 % für Tests

DCN wurde unter TensorFlow mit Keras implementiert. Die Implementierung des Modelltrainings mit DCN umfasst vier Hauptkomponenten:

* *Datenverarbeitung und Einbettung.* echte Funktionen werden durch Anwendung eines Logtransform normalisiert. Für kategorische Merkmale binden wir die Merkmale in dichte Vektoren der Dimension 6×(Category Cardinality)1/4 ein. Das Verketten aller Formationen ergibt einen Vektor der Dimension 1026.
* *Optimierung.* Wir haben die Mini-Batch stochastische Optimierung mit dem Adam Optimizer angewendet. Die Batch-Größe wurde auf 512 gesetzt. Die Batch-Normalisierung wurde auf das tiefe Netzwerk angewendet und die Gradient-Clip-Norm wurde auf 100 gesetzt.
* *Regularisierung.* Wir verwendeten das frühe Stoppen, da L2-Regularisierung oder Dropout nicht als wirksam erwiesen wurde.
* *Hyperparameter.* die Ergebnisse werden anhand einer Rastersuche über die Anzahl der ausgeblendeten Schichten, die versteckte Ebenengröße, die anfängliche Lernrate und die Anzahl der Querschichten berichtet. Die Anzahl der versteckten Schichten reichte von 2 bis 5, mit versteckten Schichtgrößen von 32 bis 1024. Bei DCN betrug die Anzahl der Querschichten von 1 bis 6. Die erste Lernrate wurde von 0.0001 auf 0.001 mit Schritten von 0.0001 abgestimmt. Alle Experimente haben einen frühen Stopp bei Trainingsschritt 150,000 durchgeführt, über den die Überlastung begann.


Neben DCN haben wir auch andere gängige Deep-Learning-Modelle für die CTR-Vorhersage getestet, einschließlich https://www.ijcai.org/proceedings/2017/0239.pdf["DeepFM"^], https://arxiv.org/pdf/1803.05170.pdf["XDeepFM"^], https://arxiv.org/abs/1810.11921["AutoInt"^], und https://arxiv.org/abs/2008.13535["DCN v2"^].



== Zur Validierung verwendete Architekturen

Für diese Validierung haben wir vier Worker-Nodes und einen Master-Node mit einem AFF A800 HA-Paar verwendet. Die Verbindung aller Cluster-Mitglieder wurde über 10-GbE-Netzwerk-Switches hergestellt.

Für diese Validierung der NetApp Spark-Lösungen haben wir drei verschiedene Storage-Controller verwendet: E5760, E5724 und AFF-A800. Die Storage-Controller der E-Series wurden mit fünf Daten-Nodes mit SAS-Verbindungen mit 12 Gbit/s verbunden. Der AFF HA-Paar-Storage Controller liefert exportierte NFS-Volumes über 10-GbE-Verbindungen zu Hadoop Worker-Nodes. Die Hadoop Cluster-Mitglieder wurden über 10-GbE-Verbindungen in den Hadoop Lösungen der E-Series, AFF und StorageGRID Hadoop verbunden.

image::apache-spark-image10.png[Zur Validierung verwendete Architekturen.]
