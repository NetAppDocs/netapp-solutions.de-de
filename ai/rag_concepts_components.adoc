---
sidebar: sidebar 
permalink: ai/rag_concepts_components.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: Enterprise RAG mit NetApp – Konzepte und Komponenten 
---
= Konzepte und Komponenten
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/




== Generative KI

KI-Systeme wie generative KI setzen bei der Entwicklung unüberwachtes oder selbstüberwachtes maschinelles Lernen auf einen großen Datensatz um. Im Gegensatz zu herkömmlichen Machine-Learning-Modellen, die Vorhersagen über einen bestimmten Datensatz treffen, sind generative KI-Modelle in der Lage, als Reaktion auf Benutzeraufforderungen neue Inhalte wie Text, Code, Bild, Video oder Audio zu generieren. Aus diesem Grund werden die Fähigkeiten eines generativen KI-Systems auch anhand der Modalität oder des Typs der verwendeten Daten klassifiziert. Diese können entweder unimodal oder multimodal sein. Ein unimodales System nimmt nur einen Eingangstyp (z. B. Nur Text oder nur Bild), während ein multimodales System mehr als einen Eingangstyp (z. B. Text, Bild und Audio), gleichzeitig verstehen und generieren Sie Inhalte über verschiedene Modalitäten hinweg. Generative KI verändert im Wesentlichen die Art und Weise, wie Unternehmen Inhalte erstellen, neue Designkonzepte generieren und Nutzen aus vorhandenen Daten ziehen.



=== Large Language Models (LLMs)

LLMs sind Deep-Learning-Modelle, die auf riesige Datenmengen vortrainiert sind, die unter anderem Text erkennen und generieren können. LLMs begannen als Teilmenge generativer KI, die sich hauptsächlich auf die Sprache konzentrierte, aber diese Unterschiede werden langsam verblassen, da multimodale LLMs weiterhin auftauchen. Der zugrunde liegende Transformator in einem LLM führt eine neue Netzwerkarchitektur ein, die nicht RNN oder CNN ist. Es verfügt über eine Reihe von neuronalen Netzen, die aus einem Encoder und einem Decoder bestehen, was hilft, Bedeutung aus einer Sequenz von Text zu extrahieren und die Beziehung zwischen Wörtern zu verstehen. LLMs können auf natürliche menschliche Sprache reagieren und mithilfe von Datenanalysen eine unstrukturierte Frage beantworten. LLMs können jedoch nur so zuverlässig sein wie die Daten, die sie aufnehmen, wodurch sie anfällig für Halluzinationen sind, die auf die Herausforderungen des Garbage-in-Garbage-Out zurückzuführen sind. Wenn LLMs mit falschen Informationen versorgt werden, können sie als Reaktion auf Benutzeranfragen ungenaue Ausgaben generieren, um einfach die Erzählung zu passen, die es baut. Unsere evidenzbasierte Forschung legt nahe, dass KI-Ingenieure auf verschiedene Methoden angewiesen sind, um diesen Halluzinationen entgegenzuwirken, zum einen durch Guardrails, die ungenaue Ergebnisse begrenzen, und zum anderen durch Feinabstimmung und Transfer des Lernens mit kontextuell relevanten Qualitätsdaten durch Techniken wie RAG.



=== Retrieval Augmented Generation (RAG)

LLMs werden auf große Datenmengen trainiert, sind jedoch nicht in Bezug auf Ihre Daten geschult. RAG löst dieses Problem, indem Sie Ihre Daten zu den Daten hinzufügen, auf die LLMs bereits Zugriff haben. Mit RAG können Kunden die Leistungsfähigkeit eines LLM nutzen, der auf ihre Daten geschult ist. Auf diese Weise können sie Informationen abrufen und so kontextbezogene Informationen für generative KI-Nutzer bereitstellen. RAG ist eine Technik des maschinellen Lernens, ein architektonischer Ansatz, der dazu beitragen kann, Halluzinationen zu reduzieren und die Effizienz und Zuverlässigkeit von LLMs zu verbessern, die Entwicklung von KI-Anwendungen zu beschleunigen und das Sucherlebnis im Unternehmen zu verbessern.



=== Ragas

Es gibt bereits vorhandene Tools und Frameworks, mit denen Sie RAG-Pipelines aufbauen können. Die Evaluierung und Quantifizierung Ihrer Pipeline-Performance kann jedoch schwierig sein. Hier kommt Ragas (RAG Assessment) ins Spiel. Ragas ist ein Framework, das Ihnen hilft, Ihre RAG-Pipelines zu bewerten. Ragas hat das Ziel, einen offenen Standard zu schaffen und Entwicklern die Tools und Techniken zur Verfügung zu stellen, um kontinuierliches Lernen in ihren RAG-Anwendungen zu nutzen. Weitere Informationen finden Sie unter https://docs.ragas.io/en/stable/getstarted/index.html["Starten Sie mit Ragas"^]



== Llama 3

Metas Llama 3, ein Modell für Decodertransformatoren, ist ein offen zugängliches, vortrainiertes Großsprachmodell (LLM). Llama 3, das auf über 15 Billionen Datentoken trainiert wurde, ist ein Game Changer im Bereich des natürlichen Sprachverständnisses (NLU). Es zeichnet sich durch kontextbezogenes Verständnis und komplexe Aufgaben wie Übersetzung und Dialoge aus. Llama 3 ist in zwei Größen erhältlich: 8 B für effiziente Implementierung und Entwicklung und 70 B für große native KI-Applikationen. Kunden können Llama 3 über Vertex AI in Google Cloud, auf Azure über Azure AI Studio und auf AWS über Amazon SageMaker implementieren.

In unserer Validierung haben wir das Llama-Modell von Meta mit NVIDIA Nemo™ Microservices, mit NVIDIA A100 GPUs beschleunigtem Computing in einer NVIDIA DGX-Instanz, implementiert, um einen generativen KI-Anwendungsfall anzupassen und zu bewerten und gleichzeitig RAG (Retrieval-Augmented Generation) in lokalen Applikationen zu unterstützen.



== Open-Source-Frameworks

Die folgenden zusätzlichen Informationen über Open-Source-Technologien können in Abhängigkeit von Ihrer Implementierung von Bedeutung sein.



=== LangChain

LangChain ist ein Open-Source-Integrations-Framework für die Entwicklung von Anwendungen, die mit großen Sprachmodellen (LLMs) betrieben werden. Kunden können RAG-Anwendungen effizient erstellen, wie es mit Document Loader, VectorStores und verschiedenen anderen Paketen kommt, wodurch Entwickler die Flexibilität haben, komplexe Workflows zu erstellen. Sie können Apps mit LangSmith auch inspizieren, überwachen und auswerten, um mit LangServe jede beliebige Kette kontinuierlich zu optimieren und in eine REST-API zu integrieren. LangChain kodiert Best Practices für RAG-Anwendungen und bietet Standardschnittstellen für verschiedene Komponenten, die für die Erstellung von RAG-Anwendungen erforderlich sind.



=== LlamaIndex

LlamaIndex ist ein einfaches, flexibles Datenframework für die Verbindung von benutzerdefinierten Datenquellen mit LLM-basierten Anwendungen (Large Language Model). Sie ermöglicht die Aufnahme von Daten aus APIs, Datenbanken, PDFs und mehr über flexible Datenverbindungen. LLLMs wie Llama 3 und GPT-4 sind bereits für umfangreiche öffentliche Datensätze vorbereitet und ermöglichen sofort eine unglaubliche natürliche Sprachverarbeitung. Ihr Nutzen ist jedoch ohne Zugriff auf Ihre eigenen privaten Daten beschränkt. LlamaIndex bietet äußerst beliebte Python- und TypeScript-Bibliotheken und ist führend in der Branche bei Retrieval-Augmented Generation (RAG)-Techniken.



== NVIDIA Nemo Microservices

NVIDIA Nemo ist eine End-to-End-Plattform zur Erstellung und Anpassung generativer KI-Modelle der Enterprise-Klasse, die überall, in der Cloud und in Datacentern implementiert werden können. Nemo bietet Microservices, die den generativen KI-Entwicklungs- und Implementierungsprozess bedarfsgerecht vereinfachen und Unternehmen die Möglichkeit geben, LLMs mit ihren Enterprise-Datenquellen zu verbinden. Zum Zeitpunkt dieser Erstellung sind die Nemo Microservices über ein Early-Access-Programm von NVIDIA verfügbar.



=== NVIDIA Nemo Inferenz Microservices (NIMS)

NVIDIA NIMS, ein Teil von NVIDIA AI Enterprise, bietet einen optimierten Weg zur Entwicklung KI-gestützter Enterprise-Applikationen und zur Implementierung von KI-Modellen in der Produktion. NIM ist ein Container-Inferenz-Mikroservice, der branchenübliche APIs, domänenspezifischen Code, optimierte Inferenzengines und Unternehmenslaufzeit umfasst.



=== NVIDIA Nemo Retriever

NVIDIA Nemo Retriever, der neueste Service im NVIDIA Nemo Framework, optimiert das Einbetten und Abrufen von RAG, um für höhere Genauigkeit und effizientere Reaktionen zu sorgen. NVIDIA Nemo Retriever ist ein Service für den Abruf von Informationen, der lokal und in der Cloud bereitgestellt werden kann. Unternehmen erhalten so einen sicheren und vereinfachten Weg zur Integration von RAG-Funktionen der Enterprise-Klasse in ihre benutzerdefinierten KI-Produktionsapplikationen.



== NVIDIA Enterprise RAG LLM Operator

Der NVIDIA Enterprise Retrieval Augmented Generation (RAG) Large Language Model (LLM) Operator ermöglicht die zum Ausführen von RAG-Pipelines in Kubernetes erforderlichen Softwarekomponenten und -Services. Sie ermöglicht einen frühzeitigen Zugriff auf einen Betreiber, der den Lebenszyklus der Schlüsselkomponenten für RAG-Pipelines verwaltet, wie z. B. NVIDIA Inference Microservice und NVIDIA Nemo Retriever Embedding Microservice. Weitere Informationen finden Sie unter https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["NVIDIA Enterprise RAG LLM Operator"^]



== Vektordatenbank



=== PostgreSQL: Pgvektor

Mit seinen nativen Bindungen für viele klassische ML-Algorithmen wie XGBoost ist maschinelles Lernen mit SQL nicht etwas Neues für PostgreSQL. Mit der Veröffentlichung von pgvektor, einer Open-Source-Erweiterung für die Vektorähnlichkeitssuche, hat PostgreSQL in letzter Zeit die Möglichkeit, ML-generierte Embeddings zu speichern und zu durchsuchen, eine Funktion, die für KI-Anwendungsfälle und Anwendungen mit LLMs nützlich ist.

Die standardmäßige Beispiel-Pipeline in unserer Validierung durch den NVIDIA Enterprise RAG LLM-Operator startet die pgvektor-Datenbank in einem Pod. Der Abfrageserver stellt dann eine Verbindung zur pgvektor-Datenbank her, um die Einbettungen zu speichern und abzurufen. Die Chat-bot-Webanwendung und der Abfrage-Server kommunizieren mit den Microservices und der Vektor-Datenbank, um auf Benutzeransagen zu reagieren.



=== Milvus

Milvus ist eine vielseitige Vektordatenbank, die ähnlich wie MongoDB eine API bietet. Sie zeichnet sich durch die Unterstützung einer Vielzahl von Datentypen und Features wie Multi-Vektorisierung aus und ist damit eine beliebte Wahl für Data Science und Machine Learning. Es kann mehr als eine Milliarde Einbettungsvektoren speichern, indizieren und verwalten, die von Deep Neural Networks (DNN)- und Machine Learning (ML)-Modellen generiert werden. Kunden können eine RAG-Anwendung mit Nvidia NIM & Nemo Microservice und Milvus als Vektordatenbank erstellen. Sobald der NVIDIA Nemo Container erfolgreich zur Einbettung der Generation bereitgestellt wurde, kann der Milvus Container zum Speichern dieser Einbettungen bereitgestellt werden. Weitere Informationen zu Vektordatenbanken und NetApp finden Sie unter https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["Referenzarchitektur – Vector Datenbanklösung mit NetApp"^].



=== Apache Cassandra

Apache Cassandra®, eine Open-Source-NoSQL, hochskalierbare und hochverfügbare Datenbank. Es wird mit Vektorsuchfunktionen ausgeliefert und unterstützt Vektordatentypen und Vektorähnlichkeitssuche, besonders nützlich für KI-Anwendungen, die LLMs und private RAG-Pipelines umfassen.

NetApp Instaclustr bietet einen vollständig gemanagten Service für Apache Cassandra, der entweder in der Cloud oder vor Ort gehostet wird. Damit können NetApp-Kunden einen Apache Cassandra® Cluster bereitstellen und mithilfe von C#, Node.js, AWS PrivateLink und verschiedenen anderen Optionen über die Instaclustr-Konsole oder die Instaclstr-Bereitstellungs-API eine Verbindung zum Cluster herstellen.

Darüber hinaus fungiert NetApp ONTAP als persistenter Storage-Provider für einen Container-Apache Cassandra Cluster, der auf Kubernetes ausgeführt wird. NetApp Astra Control erweitert die Vorteile von ONTAP für das Datenmanagement nahtlos auf datenintensive Kubernetes-Applikationen wie Apache Cassandra. Weitere Informationen hierzu finden Sie unter https://cloud.netapp.com/hubfs/SB-4134-0321-DataStax-Cassandra-Guide%20(1).pdf["Applikationsspezifisches Datenmanagement für DataStax Enterprise mit NetApp Astra Control und ONTAP Storage"^]



=== NetApp Instaclustr

Instaclustr unterstützt Unternehmen bei der Bereitstellung von Anwendungen im großen Maßstab durch Unterstützung ihrer Dateninfrastruktur über die SaaS-Plattform für Open-Source-Technologien. Generative KI-Entwickler, die semantisches Verständnis in ihre Suchanwendungen einbetten wollen, haben eine Vielzahl von Optionen. Instaclustr für Postgres unterstützt pgvektor-Erweiterungen. Instaclustr für OpenSearch unterstützt die Vektorsuche zum Abrufen relevanter Dokumente basierend auf Eingabeabfragen und den nächstgelegenen Neighbor-Funktionen. Instaclustr für Redis kann Vektordaten speichern, Vektoren abrufen und Vektorsuchen durchführen. Weitere Informationen finden Sie unter https://www.instaclustr.com/platform/["Die Instaclustr Plattform von NetApp"^]



== NetApp BlueXP

NetApp BlueXP vereint alle Storage- und Datenservices von NetApp in einem einzigen Tool zum Erstellen, Schützen und Überwachen Ihrer Hybrid-Multi-Cloud-Daten. Sie bietet eine einheitliche Arbeitsumgebung für Storage- und Datenservices lokal und in Cloud-Umgebungen und ermöglicht durch AIOps eine einfache Betriebsabläufe. Zudem bieten sie flexible Nutzungsmodelle und integrierten Schutz, die in der heutigen Cloud-orientierten Welt erforderlich sind.



== NetApp Cloud Insights

NetApp Cloud Insights ist ein Tool für das Monitoring der Cloud-Infrastruktur, mit dem Sie Ihre gesamte Infrastruktur im Blick haben. Es überwacht nicht nur alle Ressourcen, die in Public Clouds und privaten Datacentern liegen, sondern hilft auch dabei, Fehler aufzuspüren und den Ressourceneinsatz zu optimieren. Cloud Insights Cloud Insights bietet Sichtbarkeit der gesamten Infrastruktur und aller Anwendungen von Hunderten von Datensammlern für heterogene Infrastrukturen und Workloads, einschließlich Kubernetes, an einer zentralen Stelle. Weitere Informationen finden Sie unter https://docs.netapp.com/us-en/cloudinsights/index.html["Welche Vorteile bietet Cloud Insights für mich?"^]



== NetApp StorageGRID

NetApp StorageGRID ist eine Suite für softwaredefinierten Objekt-Storage, die eine Vielzahl von Anwendungsfällen in Public-, Private- und Hybrid-Multi-Cloud-Umgebungen unterstützt. StorageGRID bietet nicht nur nativen Support für die Amazon S3-API, sondern auch branchenführende Innovationen wie automatisiertes Lifecycle Management. Damit können Sie unstrukturierte Daten kostengünstig über längere Zeiträume hinweg speichern, sichern, schützen und aufbewahren.



== NetApp Spot

Spot by NetApp automatisiert und optimiert Ihre Cloud-Infrastruktur in AWS, Azure oder Google Cloud und bietet auf SLA basierende Verfügbarkeit und Performance zu minimalen Kosten. Spot verwendet Algorithmen für maschinelles Lernen und Analysen, mit denen Sie Spot-Kapazität für Produktion und geschäftskritische Workloads nutzen können. Kunden, die GPU-basierte Instanzen ausführen, können von Spot profitieren und ihre Computing-Kosten senken.



== NetApp ONTAP

ONTAP 9, die jüngste Generation der Storage-Managementsoftware von NetApp, ermöglicht Unternehmen eine Modernisierung der Infrastruktur und den Übergang zu einem Cloud-fähigen Datacenter. Dank der erstklassigen Datenmanagementfunktionen lassen sich mit ONTAP sämtliche Daten mit einem einzigen Toolset managen und schützen, ganz gleich, wo sich diese Daten befinden. Zudem können Sie die Daten problemlos dorthin verschieben, wo sie benötigt werden: Zwischen Edge, Core und Cloud. ONTAP 9 umfasst zahlreiche Funktionen, die das Datenmanagement vereinfachen, geschäftskritische Daten beschleunigen und schützen und Infrastrukturfunktionen der nächsten Generation über Hybrid-Cloud-Architekturen hinweg ermöglichen.



=== Vereinfachtes Datenmanagement

Für den Enterprise IT-Betrieb und die Data Scientists spielt Datenmanagement eine zentrale Rolle, damit für KI-Applikationen die entsprechenden Ressourcen zum Training von KI/ML-Datensätzen verwendet werden. Die folgenden zusätzlichen Informationen über NetApp Technologien sind bei dieser Validierung nicht im Umfang enthalten, können jedoch je nach Ihrer Implementierung relevant sein.

Die ONTAP Datenmanagement-Software umfasst die folgenden Funktionen, um den Betrieb zu optimieren und zu vereinfachen und damit Ihre Gesamtbetriebskosten zu senken:

* Inline-Data-Compaction und erweiterte Deduplizierung: Data-Compaction reduziert den ungenutzten Speicherplatz in Storage-Blöcken, während Deduplizierung die effektive Kapazität deutlich steigert. Dies gilt für lokal gespeicherte Daten und für Daten-Tiering in die Cloud.
* Minimale, maximale und adaptive Quality of Service (AQoS): Durch granulare QoS-Einstellungen (Quality of Service) können Unternehmen ihre Performance-Level für kritische Applikationen auch in Umgebungen mit vielen unterschiedlichen Workloads garantieren.
* NetApp FabricPool: Bietet automatisches Tiering von „kalten“ Daten in Private- und Public-Cloud-Storage-Optionen, einschließlich Amazon Web Services (AWS), Azure und NetApp StorageGRID Storage-Lösung. Weitere Informationen zu FabricPool finden Sie unter https://www.netapp.com/pdf.html?item=/media/17239-tr4598pdf.pdf["TR-4598: FabricPool Best Practices"^].




=== Beschleunigung und Sicherung von Daten

ONTAP bietet überdurchschnittliche Performance und Datensicherung, erweitert diese Funktionen auf folgende Weise:

* Performance und niedrige Latenz: ONTAP bietet höchstmöglichen Durchsatz bei geringstmöglicher Latenz.
* Datensicherung ONTAP verfügt über integrierte Funktionen für die Datensicherung mit zentralem Management über alle Plattformen hinweg.
* NetApp Volume Encryption (NVE) ONTAP bietet native Verschlüsselung auf Volume-Ebene und unterstützt sowohl Onboard- als auch externes Verschlüsselungsmanagement.
* Multi-Faktor- und Multi-Faktor-Authentifizierung – ONTAP ermöglicht die gemeinsame Nutzung von Infrastrukturressourcen mit höchstmöglicher Sicherheit.




=== Zukunftssichere Infrastruktur

ONTAP bietet folgende Funktionen, um anspruchsvolle und sich ständig ändernde Geschäftsanforderungen zu erfüllen:

* Nahtlose Skalierung und unterbrechungsfreier Betrieb. Mit ONTAP sind das Hinzufügen von Kapazitäten zu bestehenden Controllern und das Scale-out von Clustern unterbrechungsfrei möglich. Kunden können Upgrades auf die neuesten Technologien wie NVMe und 32 GB FC ohne teure Datenmigrationen oder Ausfälle durchführen.
* Cloud-Anbindung: ONTAP ist die Storage-Managementsoftware mit der umfassendsten Cloud-Integration und bietet Optionen für softwaredefinierten Storage und Cloud-native Instanzen in allen Public Clouds.
* Integration in moderne Applikationen: ONTAP bietet Datenservices der Enterprise-Klasse für Plattformen und Applikationen der neuesten Generation, wie autonome Fahrzeuge, Smart Citys und Industrie 4.0, auf derselben Infrastruktur, die bereits vorhandene Unternehmensanwendungen unterstützt.




== Amazon FSX für NetApp ONTAP

Amazon FSX for NetApp ONTAP ist ein vollständig gemanagter AWS-Service direkt vom Erstanbieter, der äußerst zuverlässigen, skalierbaren, hochperformanten und funktionsreichen File-Storage auf Basis des beliebten ONTAP-Filesystems von NetApp bietet. FSX für ONTAP kombiniert die bekannten Funktionen, Performance, Funktionen und API-Vorgänge von NetApp Filesystemen mit der Agilität, Skalierbarkeit und Einfachheit eines vollständig gemanagten AWS Service.



== Azure NetApp Dateien

Azure NetApp Files ist ein Azure-nativer, hochperformanter File-Storage-Service der Enterprise-Klasse direkt vom Erstanbieter. Die Lösung unterstützt SMB-, NFS- und duale Protokoll-Volumes und ist für Anwendungsfälle wie:

* Dateifreigabe:
* Home Directorys:
* Datenbanken.
* High-Performance-Computing.
* Generative KI:




== Google Cloud NetApp Volumes

Google Cloud NetApp Volumes ist ein vollständig gemanagter, Cloud-basierter Storage-Service, der erweiterte Datenmanagementfunktionen und hochskalierbare Performance bietet. Von NetApp gehostete Daten können in RAG-Operationen (Retrieval-Augmented Generation) für die Google-Vertex KI-Plattform in einer vorab angezeigten Toolkit-Referenzarchitektur verwendet werden.



== NetApp Astra Trident

Astra Trident ermöglicht die Nutzung und das Management von Storage-Ressourcen über alle gängigen NetApp Storage-Plattformen hinweg, in der Public Cloud oder lokal, einschließlich ONTAP (AFF, FAS, Select, Cloud, Amazon FSX for NetApp ONTAP), Element Software (NetApp HCI, SolidFire), Azure NetApp Files Service und Cloud Volumes Service auf Google Cloud. Astra Trident ist ein CSI-konformer dynamischer Storage-Orchestrator, der sich nativ in Kubernetes integrieren lässt.



== Kubernetes

Kubernetes ist eine ursprünglich von Google entwickelte Open-Source-Plattform zur Container-Orchestrierung, die jetzt von der Cloud Native Computing Foundation (CNCF) verwaltet wird. Kubernetes unterstützt die Automatisierung von Implementierungs-, Management- und Skalierungsfunktionen für Container-Applikationen und ist die dominierende Plattform für die Container-Orchestrierung in Enterprise-Umgebungen.
