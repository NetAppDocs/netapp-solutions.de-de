---
sidebar: sidebar 
permalink: ai/rag_nemo_deployment.html 
keywords: RAG, Retrieval Augmented Generation, NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NeMo, NIM, NIMS, Hybrid, Hybrid Cloud, Hybrid Multicloud, NetApp ONTAP, FlexCache, SnapMirror, BlueXP 
summary: Enterprise RAG mit NetApp – Implementierung von Nemo Microservices 
---
= Implementierung von Nemo Microservices
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./../media/


[role="lead"]
In diesem Abschnitt werden die Aufgaben beschrieben, die zur Implementierung von NVIDIA Nemo Microservices zusammen mit NetApp Storage ausgeführt werden müssen. Die NVIDIA Nemo Microservices werden über den bereitgestellt link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/index.html["NVIDIA Enterprise RAG LLM Operator"].



== Voraussetzungen

Bevor Sie die in diesem Abschnitt beschriebenen Schritte ausführen, gehen wir davon aus, dass Sie bereits die folgenden Aufgaben ausgeführt haben:

* Sie haben bereits einen funktionierenden Kubernetes-Cluster und führen eine Version von Kubernetes aus, die vom NVIDIA Enterprise RAG LLM Operator unterstützt wird. Eine Liste der unterstützten Kubernetes-Versionen finden Sie im link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html["RAG LLM-Betriebsdokumentation."] Dieser Kubernetes-Cluster kann entweder lokal oder in der Cloud eingesetzt werden.
* Ihr Kubernetes-Cluster umfasst mindestens drei GPUs, die vom NVIDIA Enterprise RAG LLM Operator unterstützt werden. Eine Liste der unterstützten GPUs finden Sie im link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/platform-support.html["RAG LLM-Betriebsdokumentation."]
* Sie haben NetApp Astra Trident bereits in Ihrem Kubernetes-Cluster installiert und konfiguriert. Weitere Informationen zu Astra Trident finden Sie im link:https://docs.netapp.com/us-en/trident/index.html["Astra Trident-Dokumentation"]. Diese Lösung ist mit jeder physischen NetApp Storage Appliance, softwaredefinierten Instanz oder Cloud-Service kompatibel, die von Trident unterstützt wird.




== Verwenden Sie den NVIDIA Enterprise RAG LLM Operator, um NVIDIA Nemo Microservices bereitzustellen

. Wenn der NVIDIA GPU Operator noch nicht in Ihrem Kubernetes-Cluster installiert ist, installieren Sie den NVIDIA GPU Operator, indem Sie die Anweisungen in befolgen link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-nvidia-gpu-operator["RAG LLM-Betriebsdokumentation."]
. Installieren Sie den NVIDIA Enterprise RAG LLM Operator, indem Sie die Anweisungen in befolgen link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/install.html#install-the-rag-llm-operator["RAG LLM-Betriebsdokumentation."]
. Erstellen Sie mithilfe des NVIDIA Enterprise RAG LLM-Operators eine RAG-Pipeline. Befolgen Sie dazu die Anweisungen in link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/pipelines.html["RAG LLM-Betriebsdokumentation."]
+
** Geben Sie bei der Angabe einer StorageClass eine StorageClass an, die Astra Trident verwendet.
** Standardmäßig stellt die RAG-Pipeline eine neue pgvektorielle Datenbank bereit, die als Vektorspeicher/Wissensdatenbank für die RAG-Bereitstellung dient. Wenn Sie stattdessen eine vorhandene pgvecor- oder Milvus-Instanz verwenden möchten, befolgen Sie die Anweisungen im link:https://docs.nvidia.com/ai-enterprise/rag-llm-operator/0.4.1/vector-database.html["RAG LLM-Betriebsdokumentation."] Weitere Informationen zum Ausführen einer Vektordatenbank mit NetApp finden Sie im link:https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-solution-with-netapp.html["Dokumentation der NetApp Vector Datenbanklösung."]



