---
sidebar: sidebar 
permalink: ai/ai-edge-summary.html 
keywords:  
summary:  
---
= Zusammenfassung
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Verschiedene neue Applikationsszenarien, etwa durch erweiterte Treiberassistenzsysteme (ADAS), Industrie 4.0, Smart Cities und Internet of Things (IoT), erfordern die Verarbeitung von kontinuierlichen Datenströmen bei nahezu null Latenz. In diesem Dokument wird eine Computing- und Storage-Architektur beschrieben, um GPU-basierte Inferenz (KI) auf NetApp Storage-Controllern und Lenovo ThinkSystem Servern in einer Edge-Umgebung zu implementieren, die diese Anforderungen erfüllt. Dieses Dokument bietet auch Performance-Daten für die branchenübliche MLPerf Inferenz-Benchmark. Hierbei werden verschiedene Inferenzaufgaben auf Edge-Servern mit NVIDIA T4-GPUs evaluiert. Wir untersuchen die Performance von Offline-, Single-Stream- und Multistream-Inferenzszenarien und zeigen, dass die Architektur mit einem kostengünstigen Shared-Networked Storage-System eine hohe Performance aufweist und einen zentralen Datenstandort- und Modellmanagement für mehrere Edge-Server ermöglicht.
